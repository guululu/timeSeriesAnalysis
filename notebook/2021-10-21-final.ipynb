{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from functools import partial\n",
    "from typing import Dict\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "from main.utils.data import split_dataset, raw_data_preparation\n",
    "from main.utils.utils import optimization_process\n",
    "from main.settings import train_validation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 365 Days Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = 52\n",
    "n_gap = 7\n",
    "n_out = 4\n",
    "scale = 100000\n",
    "\n",
    "n_splits = train_validation_config['n_splits']\n",
    "max_train_size = train_validation_config['max_train_size']\n",
    "\n",
    "df = raw_data_preparation()\n",
    "\n",
    "daily_data = df[['A']]\n",
    "daily_data.fillna(0, inplace=True)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits + (n_out - 1), test_size=7, max_train_size=max_train_size)\n",
    "\n",
    "predictions_365 = []\n",
    "observations = []\n",
    "\n",
    "for train_index, val_index in tscv.split(daily_data):\n",
    "    if (train_index[-1] + n_out * 7 + 1) <= len(daily_data):\n",
    "        val_index = np.arange(train_index[-1] + 1, train_index[-1] + n_out * 7 + 1)\n",
    "    \n",
    "        train = daily_data.iloc[train_index] / scale\n",
    "        val = daily_data.iloc[val_index] / scale\n",
    "\n",
    "        yhat = [train[-(n_gap * 7 + 365):-(n_gap * 7)].values.mean() * 7] * n_out\n",
    "        y = np.array(np.split(val.values.reshape(-1,), n_out)).sum(axis=1)\n",
    "\n",
    "        predictions_365.append(yhat)\n",
    "        observations.append(y)\n",
    "    \n",
    "predictions_365 = np.array(predictions_365)\n",
    "observations = np.array(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mse = {mean_squared_error(observations, predictions_365)}')\n",
    "\n",
    "diff = observations - predictions_365\n",
    "diff_square = np.power(diff, 2)\n",
    "\n",
    "baseline_365 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA Bench Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()\n",
    "df['week'] = [idx.week for idx in df.index]\n",
    "df['year'] = [idx.year for idx in df.index]\n",
    "\n",
    "df_A_year_week = df.groupby(['week', 'year'], as_index=False)['A'].sum()\n",
    "\n",
    "df_A_year_week.sort_values(by=['year', 'week'], inplace=True)\n",
    "\n",
    "df_A_year_week.reset_index(drop=True, inplace=True)\n",
    "\n",
    "A_week = df_A_year_week['A'].values\n",
    "\n",
    "\n",
    "def sarima_moving_window(p, d, q, season_p, season_d, season_q, s):\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits + (n_out - 1), test_size=1, max_train_size=max_train_size//7)\n",
    "\n",
    "    predictions = []\n",
    "    observations = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(A_week):\n",
    "\n",
    "        if (train_index[-1] + n_out + 1) <= len(A_week):\n",
    "            val_index = np.arange(train_index[-1] + 1, train_index[-1] + n_out + 1)\n",
    "\n",
    "            train = A_week[train_index] / scale\n",
    "            val = A_week[np.concatenate((train_index[-(n_gap):], val_index))] / scale\n",
    "\n",
    "            sarima = SARIMAX(train[:-n_gap], order=(p,d,q), seasonal_order=(season_p, season_d, season_q, s))\n",
    "            model = sarima.fit()\n",
    "            p_weekly = model.forecast(steps=(n_gap + n_out))[-n_out:]\n",
    "            o_weekly = A_week[val_index] / scale\n",
    "\n",
    "            predictions.append(p_weekly)\n",
    "            observations.append(o_weekly)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- season_P = 1\n",
    "- s = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sarima_p1_s11, observations = sarima_moving_window(0, 0, 0, 1, 0, 0, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# ax.plot(observations.reshape(-1, ), c='b', label='observations')\n",
    "# ax.plot(predictions_sarima_p1_s11.reshape(-1, ), c='r', label='predictions')\n",
    "\n",
    "# time = [str(d)[:10] for d in df.iloc[-week * 7:].index[::7].values]\n",
    "\n",
    "# ax.set_title('SARIMA(P=1, S=11)', fontsize=24)\n",
    "# ax.set_xlim(0, 51)\n",
    "# ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "# ax.set_xticklabels(labels=[0] + time[::4], rotation=45)\n",
    "# ax.set_xlabel('Time', fontsize=24)\n",
    "# ax.set_ylabel('A weekly consumption (100000)', fontsize=24)\n",
    "\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mse = {mean_squared_error(observations, predictions_sarima_p1_s11)}')\n",
    "\n",
    "diff = observations - predictions_sarima_p1_s11\n",
    "diff_square = np.power(diff, 2)\n",
    "\n",
    "arima_p1_s11 = np.sqrt(diff_square.mean(axis=1))\n",
    "\n",
    "# print(f'weekly mse = {np.sqrt(diff_square.mean(axis=0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_p1_s11.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q=1\n",
    "- S=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sarima_q1_s11, observations = sarima_moving_window(p=0, d=0, q=0, season_p=0, season_d=0, season_q=1, s=11)\n",
    "print(f'\\nmse = {mean_squared_error(observations, predictions_sarima_q1_s11)}')\n",
    "diff = observations - predictions_sarima_q1_s11\n",
    "diff_square = np.power(diff, 2)\n",
    "arima_q1_s11 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P=1\n",
    "- Q=1\n",
    "- S=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sarima_p1_q1_s11, observations = sarima_moving_window(p=0, d=0, q=0, season_p=1, season_d=0, season_q=1, s=11)\n",
    "\n",
    "print(f'\\nmse = {mean_squared_error(observations, predictions_sarima_p1_q1_s11)}')\n",
    "\n",
    "diff = observations - predictions_sarima_p1_q1_s11\n",
    "diff_square = np.power(diff, 2)\n",
    "\n",
    "arima_p1_q1_s11 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P=1\n",
    "- S=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sarima_p1_s8, observations = sarima_moving_window(p=0, d=0, q=0, season_p=1, season_d=0, season_q=0, s=8)\n",
    "print(f'\\nmse = {mean_squared_error(observations, predictions_sarima_p1_s8)}')\n",
    "diff = observations - predictions_sarima_p1_s8\n",
    "diff_square = np.power(diff, 2)\n",
    "arima_p1_s8 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P=1\n",
    "- S=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sarima_p1_s4, observations = sarima_moving_window(p=0, d=0, q=0, season_p=1, season_d=0, season_q=0, s=4)\n",
    "print(f'\\nmse = {mean_squared_error(observations, predictions_sarima_p1_s4)}')\n",
    "diff = observations - predictions_sarima_p1_s4\n",
    "diff_square = np.power(diff, 2)\n",
    "arima_p1_s4 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28 days to 4 weeks exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from train.training_process import training_process\n",
    "from main.utils.utils import optimization_process\n",
    "from main.settings import train_validation_config, requirements\n",
    "\n",
    "n_input = requirements['n_input']\n",
    "n_out = requirements['n_out']\n",
    "n_gap = requirements['n_gap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "# from functools import partial\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# '''\n",
    "# 10.43\n",
    "\n",
    "# beta_1 = 0.6799\n",
    "# beta_2 = 0.7932\n",
    "# decoder_dense_units = 11\n",
    "# epochs = 13\n",
    "# epsilon = 0.007872\n",
    "# learning_rate = 0.002073\n",
    "# lstm_units = 128\n",
    "# '''\n",
    "\n",
    "# model_name = 'lstm_v1'\n",
    "# model_type = 'daily2weekly'\n",
    "\n",
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff'] = df['A'].diff()\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff']]\n",
    "\n",
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving window prediction\n",
    "from train.training_process import moving_window_predictions\n",
    "\n",
    "observations, predictions_lstm_v1 = moving_window_predictions(daily_data, model_name='lstm_v1', model_type='daily2weekly', lstm_units=128, decoder_dense_units=11, \n",
    "                                                              epochs=13, statistical_operation=statistical_operation, learning_rate=0.002073, beta_1=0.6799, \n",
    "                                                              beta_2=0.7932, epsilon=0.007872, n_input=n_input, n_out=n_out, n_gap=n_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mse = {mean_squared_error(observations, predictions_lstm_v1)}')\n",
    "\n",
    "diff = observations - predictions_lstm_v1\n",
    "diff_square = np.power(diff, 2)\n",
    "\n",
    "lstm_v1 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10.46\n",
    "\n",
    "beta_1 = 0.8397\n",
    "beta_2 = 0.8028\n",
    "decoder_dense_units = 19\n",
    "epochs = 5\n",
    "epsilon = 0.002189\n",
    "learning_rate = 0.002168\n",
    "lstm_units = 49\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, predictions_lstm_v2 = moving_window_predictions(daily_data, model_name='lstm_v2', model_type='daily2weekly', lstm_units=49, decoder_dense_units=19, \n",
    "                                                              epochs=5, statistical_operation=statistical_operation, learning_rate=0.002168, beta_1=0.8397, \n",
    "                                                              beta_2=0.8028, epsilon=0.002189, n_input=n_input, n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "print(f'mse = {mean_squared_error(observations, predictions_lstm_v2)}')\n",
    "\n",
    "diff = observations - predictions_lstm_v2\n",
    "\n",
    "diff_square = np.power(diff, 2)\n",
    "lstm_v2 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10.40\n",
    "\n",
    "beta_1 = 0.6191\n",
    "beta_2 = 0.8347\n",
    "decoder_dense_units = 10\n",
    "epochs = 5\n",
    "epsilon = 0.002229\n",
    "learning_rate = 0.001867\n",
    "lstm_units = 74\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, predictions_lstm_v3 = moving_window_predictions(daily_data, model_name='lstm_v3', model_type='daily2weekly', lstm_units=74, decoder_dense_units=10, \n",
    "                                                              epochs=5, statistical_operation=statistical_operation, learning_rate=0.001867, beta_1=0.6191, \n",
    "                                                              beta_2=0.8347, epsilon=0.002229, n_input=n_input, n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "print(f'mse = {mean_squared_error(observations, predictions_lstm_v3)}')\n",
    "\n",
    "diff = observations - predictions_lstm_v3\n",
    "\n",
    "diff_square = np.power(diff, 2)\n",
    "lstm_v3 = np.sqrt(diff_square.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "ax.plot(baseline_365, label='base_line_rmse')\n",
    "ax.plot(lstm_v1, label='lstm_v1_rmse')\n",
    "ax.plot(lstm_v2, label='lstm_v2_rmse')\n",
    "ax.plot(lstm_v3, label='lstm_v3_rmse')\n",
    "\n",
    "ax.set_xlabel('train/val rmse round', fontsize=18)\n",
    "ax.set_ylabel('RMSE (100,000)', fontsize=18)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 weeks to 4 weeks exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from train.training_process import training_process\n",
    "from main.utils.utils import optimization_process\n",
    "from main.settings import train_validation_config\n",
    "\n",
    "n_input = 56\n",
    "n_out = 4\n",
    "n_gap = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_v2'\n",
    "model_type = 'weekly2weekly'\n",
    "\n",
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff'] = df['A'].diff()\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff']]\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01)}\n",
    "\n",
    "\n",
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "\n",
    "training_process_fn = partial(training_process, daily_data=daily_data, model_name=model_name,\n",
    "                              model_type=model_type, n_out=n_out, n_gap=n_gap, n_input=n_input,\n",
    "                              statistical_operation=statistical_operation)\n",
    "\n",
    "optimized_parameters = optimization_process(training_process_fn, pbounds, model_name=model_name, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
