{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "Baisc API usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the boundaries of hyperparameters\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01),\n",
    "           'conv1d_filters': (24, 64),\n",
    "           'learning_rate': (0.0001, 0.003)}\n",
    "\n",
    "# fn: some function\n",
    "optimizer = BayesianOptimization(f=fn, pbounds=pbounds, random_state=1)\n",
    "\n",
    "bayesianOptimization = {'init_points': 8\n",
    "                        'n_iter': 24\n",
    "                        'acq': 'ucb'}\n",
    "  \n",
    "optimizer.maximize(\n",
    "    **bayesianOptimization\n",
    ")\n",
    "optimized_parameters = optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://cds.cern.ch/record/2702355/files/step1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://www.borealisai.com/media/filer_public_thumbnails/filer_public/98/ec/98ece68f-85b6-4f5e-bb3e-f56dfd31fc47/t9_figure3.png__6000x4333_q85_subject_location-3000%2C2171_subsampling-2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/09-token-vs-character-embeddings.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder LSTM + Time \n",
    "\n",
    "We consider a simple LSTM with time info as categorical inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from main.utils.data import split_dataset, raw_data_preparation\n",
    "from main.utils.utils import to_supervised, optimization_process\n",
    "from main.settings import holiday\n",
    "from train.training_process_weekly2weekly import training_process\n",
    "\n",
    "# prediction for four weeks\n",
    "n_out = 4\n",
    "# 7 weeks gap\n",
    "n_gap = 7\n",
    "# 56 days input\n",
    "n_input = 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff'] = df['A'].diff(7)\n",
    "df.loc[df.index.isin(holiday), 'holiday'] = 1\n",
    "\n",
    "# Considering data with five features: A, C, G, A_diff, month\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff', 'holiday']] # remember the index for month\n",
    "daily_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm_v2\n",
    "\n",
    "1 year of training data\n",
    "8 weeks historical weekly consumption as input\n",
    "\n",
    "n_input = 56\n",
    "mse: 10.84  2021/10/04\n",
    "\n",
    "- beta_1: 0.9342\n",
    "- beta_2: 0.7199\n",
    "- decoder_dense_units: 8 \n",
    "- epochs: 14\n",
    "- epsilon: 0.005795\n",
    "- learning_rate: 0.002596\n",
    "- lstm_units: 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01),\n",
    "           'learning_rate': (0.0001, 0.003)}\n",
    "\n",
    "lstm_training_process_fn = partial(training_process, daily_data=daily_data,\n",
    "                                   model_name='lstm_v2', statistical_operation=statistical_operation,\n",
    "                                   scaler=None, n_out=4, n_gap=7, scale=100000, n_input=56,\n",
    "                                   conv1d_filters=None)\n",
    "lstm_v2_optimized_parameters = optimization_process(lstm_training_process_fn, pbounds)\n",
    "\n",
    "print(lstm_v2_optimized_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn_lstm_v2\n",
    "\n",
    "1 year of training data\n",
    "8 weeks historical weekly consumption as input\n",
    "\n",
    "n_input = 56\n",
    "\n",
    "mse: 10.45  2021/10/04\n",
    "\n",
    "- beta_1: 0.5\n",
    "- beta_2: 0.9998\n",
    "- conv1d_filters: 30\n",
    "- decoder_dense_units: 8\n",
    "- epochs: 20\n",
    "- epsilon: 0.008877\n",
    "- learning_rate: 0.003\n",
    "- lstm_units: 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01),\n",
    "           'conv1d_filters': (24, 64)\n",
    "          }\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "               'lstm_units': (48, 130),\n",
    "               'decoder_dense_units': (8, 20),\n",
    "               'beta_1': (0.5, 0.95),\n",
    "               'beta_2': (0.7, 0.9999),\n",
    "               'epsilon': (0.00001, 0.01),\n",
    "               'conv1d_filters': (24, 64),\n",
    "               'learning_rate': (0.0001, 0.003)}\n",
    "\n",
    "lstm_training_process_fn = partial(training_process, daily_data=daily_data,\n",
    "                                   model_name='cnn_lstm_v2', statistical_operation=statistical_operation,\n",
    "                                   scaler=None, n_out=4, n_gap=7, scale=100000, n_input=56, conv1d_filters=None)\n",
    "cnn_lstm_v2_optimized_parameters = optimization_process(lstm_training_process_fn, pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn_lstm_v2_luong\n",
    "\n",
    "1 year of training data\n",
    "8 weeks historical weekly consumption as input\n",
    "\n",
    "n_input = 56\n",
    "mse: 10.64  2021/10/04\n",
    "\n",
    "- beta_1: 0.95\n",
    "- beta_2: 0.9999\n",
    "- conv1d_filters: 42 \n",
    "- decoder_dense_units: 19 \n",
    "- epochs: 10\n",
    "- epsilon: 1e-5\n",
    "- learning_rate: 0.003 \n",
    "- lstm_units: 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01),\n",
    "           'conv1d_filters': (24, 64),\n",
    "           'learning_rate': (0.0001, 0.003)}\n",
    "\n",
    "lstm_training_process_fn = partial(training_process, daily_data=daily_data,\n",
    "                                   model_name='cnn_lstm_v2_luong', statistical_operation=statistical_operation,\n",
    "                                   scaler=None, n_out=4, n_gap=7, scale=100000, n_input=56)\n",
    "cnn_lstm_v2_luong_optimized_parameters = optimization_process(lstm_training_process_fn, pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineeering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn_lstm_v2\n",
    "\n",
    "1 year of training data\n",
    "8 weeks historical weekly consumption as input\n",
    "\n",
    "- holidays\n",
    "\n",
    "tanh activation\n",
    "\n",
    "mse: 10.28  2021/10/04\n",
    "\n",
    "- beta_1: 0.5 \n",
    "- beta_2: 0.7\n",
    "- conv1d_filters: 59 \n",
    "- decoder_dense_units: 20 \n",
    "- epochs: 20\n",
    "- epsilon: 1e-5\n",
    "- learning_rate: 0.003 \n",
    "- lstm_units: 118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff_7'] = df['A'].diff(7)\n",
    "df.loc[df.index.isin(holiday), 'holiday'] = 1\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Considering data with five features: A, C, G, A_diff\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff_7', 'holiday']]\n",
    "\n",
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "statistical_operation[4] = ['sum']\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01),\n",
    "           'conv1d_filters': (24, 64),\n",
    "           'learning_rate': (0.0001, 0.003)}\n",
    "\n",
    "lstm_training_process_fn = partial(training_process, daily_data=daily_data,\n",
    "                                   model_name='cnn_lstm_v2', statistical_operation=statistical_operation,\n",
    "                                   scaler=None, n_out=4, n_gap=7, scale=100000, n_input=56)\n",
    "cnn_lstm_v2_optimized_parameters = optimization_process(lstm_training_process_fn, pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn_lstm_v2\n",
    "\n",
    "1 year of training data\n",
    "8 weeks historical weekly consumption as input\n",
    "\n",
    "- holidays\n",
    "- A 14 days rolling mean\n",
    "\n",
    "tanh activation\n",
    "\n",
    "mse: 9.684  2021/10/04\n",
    "\n",
    "- beta_1: 0.5\n",
    "- beta_2: 0.7\n",
    "- conv1d_filters: 48 \n",
    "- decoder_dense_units: 8 \n",
    "- epochs: 20\n",
    "- epsilon: 1e-5\n",
    "- learning_rate: 0.003 \n",
    "- lstm_units: 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff_7'] = df['A'].diff(7)\n",
    "df.loc[df.index.isin(holiday), 'holiday'] = 1\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['rolling_mean'] = df['A'].rolling(14).mean()\n",
    "\n",
    "# Considering data with five features: A, C, G, A_diff\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff_7', 'holiday', 'rolling_mean']]\n",
    "\n",
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "statistical_operation[4] = ['sum']\n",
    "statistical_operation[5] = ['max']\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01),\n",
    "           'conv1d_filters': (24, 64),\n",
    "           'learning_rate': (0.0001, 0.003)}\n",
    "\n",
    "lstm_training_process_fn = partial(training_process, daily_data=daily_data,\n",
    "                                   model_name='cnn_lstm_v2', statistical_operation=statistical_operation,\n",
    "                                   scaler=None, n_out=4, n_gap=7, scale=100000, n_input=56)\n",
    "cnn_lstm_v2_optimized_parameters = optimization_process(lstm_training_process_fn, pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn_lstm_v2_luong\n",
    "\n",
    "1 year of training data\n",
    "8 weeks historical weekly consumption as input\n",
    "\n",
    "- holidays\n",
    "\n",
    "tanh activation\n",
    "\n",
    "mse: 10.39  2021/10/04\n",
    "\n",
    "- beta_1: 0.5\n",
    "- beta_2: 0.7095\n",
    "- conv1d_filters: 28\n",
    "- decoder_dense_units: 20\n",
    "- epochs: 5\n",
    "- epsilon: 1e-5\n",
    "- learning_rate: 0.001542\n",
    "- lstm_units: 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff_7'] = df['A'].diff(7)\n",
    "df.loc[df.index.isin(holiday), 'holiday'] = 1\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Considering data with five features: A, C, G, A_diff\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff_7', 'holiday']]\n",
    "\n",
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "statistical_operation[4] = ['sum']\n",
    "\n",
    "pbounds = {'epochs': (5, 20),\n",
    "           'lstm_units': (48, 130),\n",
    "           'decoder_dense_units': (8, 20),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01),\n",
    "           'conv1d_filters': (24, 64),\n",
    "           'learning_rate': (0.0001, 0.003)}\n",
    "\n",
    "lstm_training_process_fn = partial(training_process, daily_data=daily_data,\n",
    "                                   model_name='cnn_lstm_v2_luong', statistical_operation=statistical_operation,\n",
    "                                   scaler=None, n_out=4, n_gap=7, scale=100000, n_input=56)\n",
    "cnn_lstm_v2_luong_optimized_parameters = optimization_process(lstm_training_process_fn, pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn_lstm_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from train.LSTM_weekly2weekly_model import build_cnn_lstm_v2_model\n",
    "from train.training_process_weekly2weekly import evaluation_model\n",
    "\n",
    "n_out = 4\n",
    "n_gap = 7\n",
    "n_input = 56\n",
    "scale = 100000\n",
    "\n",
    "# use the optimized parameters\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.7\n",
    "conv1d_filters = 48\n",
    "decoder_dense_units = 8\n",
    "epochs = 20\n",
    "epsilon = 1e-5\n",
    "learning_rate = 0.003\n",
    "lstm_units = 113\n",
    "\n",
    "df = raw_data_preparation()\n",
    "\n",
    "# Time info\n",
    "df.loc[df.index.isin(holiday), 'holiday'] = 1\n",
    "df['A_diff_7'] = df['A'].diff(7)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['rolling_mean'] = df['A'].rolling(14).mean()\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff_7', 'holiday', 'rolling_mean']]\n",
    "\n",
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['sum']\n",
    "statistical_operation[3] = ['sum']\n",
    "statistical_operation[4] = ['sum']\n",
    "statistical_operation[5] = ['max']\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=13, test_size=n_out * 7, max_train_size=52 * 7)\n",
    "\n",
    "cnn_lstm_predictions = []\n",
    "observations = []\n",
    "\n",
    "for train_index, val_index in tscv.split(daily_data):\n",
    "    \n",
    "    train = daily_data.iloc[train_index].values / scale\n",
    "    val = daily_data.iloc[np.concatenate((train_index[-(n_gap * 7):], val_index))].values / scale\n",
    "\n",
    "    val_label = val[:, 0]\n",
    "    train_label = train[:, 0]\n",
    "    \n",
    "    val = np.array(np.split(val, len(val) / 7))\n",
    "    train = np.array(np.split(train, len(train) / 7))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "    \n",
    "    model = build_cnn_lstm_v2_model(train, train_label, n_input=n_input, lstm_units=lstm_units,\n",
    "                                    decoder_dense_units=decoder_dense_units, conv1d_filters=conv1d_filters,\n",
    "                                    optimizer=optimizer, epochs=epochs, n_out=n_out, n_gap=n_gap,\n",
    "                                    statistical_operation=statistical_operation)\n",
    "    \n",
    "    p, o = evaluation_model(model, train, val, val_label, n_input, n_out=n_out, n_gap=n_gap,\n",
    "                            statistical_operation=statistical_operation)\n",
    "    \n",
    "    observations.append(o)\n",
    "    cnn_lstm_predictions.append(p)\n",
    "    \n",
    "observations = np.concatenate(observations, axis=0)\n",
    "cnn_lstm_predictions = np.concatenate(cnn_lstm_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "ax.plot(observations.reshape(-1, ), c='b', label='observations')\n",
    "ax.plot(cnn_lstm_predictions.reshape(-1, ), c='r', label='predictions')\n",
    "\n",
    "time = [str(d)[:10] for d in daily_data.iloc[-52 * 7:].index[::7].values]\n",
    "\n",
    "ax.set_title('CNN-LSTM', fontsize=24)\n",
    "ax.set_xlim(0, 51)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "ax.set_xticklabels(labels=[0] + time[::4], rotation=45)\n",
    "ax.set_xlabel('Time', fontsize=24)\n",
    "ax.set_ylabel('A weekly consumption (100000)', fontsize=24)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(observations, cnn_lstm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = observations - cnn_lstm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_square = np.power(diff, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(diff_square.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_v2_architecture(lstm_units, decoder_dense_units, conv1d_filters,\n",
    "                                   n_inputs, n_features, n_outputs) -> Model:\n",
    "\n",
    "    tf.random.set_seed(42)\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # define model\n",
    "    main_inputs = Input(shape=(n_inputs, n_features), name='weekly_inputs')\n",
    "    x = Conv1D(filters=conv1d_filters, kernel_size=3, activation='relu', padding='same')(main_inputs)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv1D(filters=conv1d_filters, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    decoder_input = RepeatVector(n_outputs)(x)  # Repeatvector(n_out)(state_h)\n",
    "\n",
    "    y = LSTM(lstm_units, activation='tanh', return_sequences=True, dropout=0.1,\n",
    "             recurrent_dropout=0.1)(decoder_input)\n",
    "\n",
    "    y = TimeDistributed(Dense(decoder_dense_units, activation='relu'))(y)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(y)\n",
    "\n",
    "    model = Model(inputs=main_inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "scheduled_off_df = pd.DataFrame({'holiday': 'scheduled_off',\n",
    "                                 'ds': holiday,\n",
    "                                 'lower_window': -2,  # range of impact of off days\n",
    "                                 'upper_window': 2})\n",
    "\n",
    "df_prophet = df.copy()\n",
    "\n",
    "df_prophet.index.name = 'ds'  # specify timestamp index name as ds\n",
    "df_prophet.reset_index(inplace=True)  # make index into column\n",
    "df_prophet.rename(columns={'A': 'y'}, inplace=True)  # rename the target column as y\n",
    "\n",
    "df_prophet['y'] = df_prophet['y'] / scale\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=13, test_size=n_out * 7)\n",
    "\n",
    "prophet_predictions = []\n",
    "observations = []\n",
    "\n",
    "for train_index, val_index in tscv.split(df_prophet):\n",
    "    \n",
    "    df_train = df_prophet.iloc[train_index]\n",
    "    \n",
    "    m = Prophet(holidays=scheduled_off_df, \n",
    "                growth='linear',\n",
    "                changepoint_prior_scale=0.005,\n",
    "                changepoint_range=0.9,\n",
    "                yearly_seasonality=10,\n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=False)\n",
    "    \n",
    "    m.add_seasonality(name='month', period=30, fourier_order=5) \n",
    "    m.fit(df_train.iloc[:-(n_gap) * 7])\n",
    "    \n",
    "    future = m.make_future_dataframe(periods=(n_gap + n_out) * 7)\n",
    "    \n",
    "    p = m.predict(future.iloc[-(n_out) * 7:])['yhat'].values\n",
    "    \n",
    "    p_weekly = np.array(np.split(p, n_out)).sum(axis=1)\n",
    "    \n",
    "    o = df_prophet.iloc[val_index]['y'].values\n",
    "    \n",
    "    o_weekly = np.array(np.split(o, n_out)).sum(axis=1)\n",
    "    \n",
    "    prophet_predictions.append(p_weekly)\n",
    "    observations.append(o_weekly)\n",
    "    \n",
    "prophet_predictions = np.array(prophet_predictions)\n",
    "observations = np.array(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "ax.plot(observations.reshape(-1, ), c='b', label='observations')\n",
    "ax.plot(prophet_predictions.reshape(-1, ), c='r', label='predictions')\n",
    "\n",
    "time = [str(d)[:10] for d in daily_data.iloc[-52 * 7:].index[::7].values]\n",
    "\n",
    "ax.set_title('Prophet', fontsize=24)\n",
    "ax.set_xlim(0, 51)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "ax.set_xticklabels(labels=[0] + time[::4], rotation=45)\n",
    "ax.set_xlabel('Time', fontsize=24)\n",
    "ax.set_ylabel('A weekly consumption (100000)', fontsize=24)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST\n",
    "\n",
    "feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = []\n",
    "daily_data = df[['A', 'A_diff_7', 'rolling_mean']]\n",
    "\n",
    "statistical_operation = OrderedDict()\n",
    "\n",
    "statistical_operation[0] = ['sum']\n",
    "statistical_operation[1] = ['sum']\n",
    "statistical_operation[2] = ['max']\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=13, test_size=n_out * 7, max_train_size=52 * 7)\n",
    "\n",
    "for train_index, val_index in tscv.split(daily_data):\n",
    "    \n",
    "    train = daily_data.iloc[train_index].values / scale\n",
    "    val = daily_data.iloc[np.concatenate((train_index[-(n_gap * 7):], val_index))].values / scale\n",
    "    \n",
    "    val_label = val[:, 0]\n",
    "    \n",
    "    val = np.array(np.split(val, len(val) / 7))\n",
    "    train = np.array(np.split(train, len(train) / 7))\n",
    "    \n",
    "    history = [x for x in train[:-n_gap]]  # [x for x in train[:-(n_out + n_gap)]]\n",
    "\n",
    "    for i in range(len(val) - n_gap):\n",
    "        if (i + n_out + n_gap) * 7 <= len(val_label):\n",
    "            \n",
    "            data = np.array(history)\n",
    "            data = data.reshape(data.shape[0] * data.shape[1], data.shape[2])\n",
    "\n",
    "            input_weekly = []\n",
    "\n",
    "            for col_index, operations in statistical_operation.items():\n",
    "                for operation in operations:\n",
    "                    statistical_feature = eval(f'np.nan{operation}(np.array(np.split(data[-n_input:, '\n",
    "                                               f'col_index], n_input // 7)), axis=1, keepdims=True)')\n",
    "                    if np.isnan(np.sum(statistical_feature)):\n",
    "                        statistical_feature = np.nan_to_num(statistical_feature)\n",
    "                    input_weekly.append(statistical_feature)\n",
    "            \n",
    "            input_weekly = np.expand_dims(np.concatenate(input_weekly, axis=1), 0)\n",
    "            \n",
    "            model_features.append(input_weekly)\n",
    "            \n",
    "model_features = np.concatenate(model_features, axis=0)\n",
    "\n",
    "model_features = model_features[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, timesteps, n_features = model_features.shape\n",
    "\n",
    "model_features = model_features.reshape(-1, timesteps * n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = []\n",
    "\n",
    "for a, cl_predictions, pr_predictions, observation in zip(model_features, cnn_lstm_predictions, prophet_predictions, observations):\n",
    "    \n",
    "    weekly_cl_diff = 0\n",
    "    weekly_pr_diff = 0\n",
    "    \n",
    "    for ix, values in enumerate(zip(cl_predictions, pr_predictions, observation)):\n",
    "        \n",
    "#         data_.append(list(a) + [ix] + list(values))\n",
    "        pred_diff = values[0] - values[1]\n",
    "        \n",
    "        if ix == 0:\n",
    "            data_.append([pred_diff, 0, 0] + list(values))\n",
    "        else:\n",
    "            data_.append([pred_diff, values[0] - cl_prediction_prev, values[1] - pr_prediction_prev] + list(values))\n",
    "    \n",
    "        cl_prediction_prev = values[0]\n",
    "        pr_prediction_prev = values[1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features = pd.DataFrame(data=data_, columns=['A_D2', 'A_diff_7_D2', 'rolling_mean_D2', \n",
    "#                                                 'A_D1', 'A_diff_7_D1', 'rolling_mean_D1',\n",
    "#                                                 'week', 'cnn_lstm_pred', 'prophet_pred', 'target'])\n",
    "\n",
    "df_features = pd.DataFrame(data=data_, columns=['diff_pred', 'cl_weekly_diff', 'pr_weekly_diff', \n",
    "                                                'cnn_lstm_pred', 'prophet_pred', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "model = XGBRegressor(n_estimators=50, random_state=0, n_jobs=-1, objective='reg:squarederror', max_depth=3, learning_rate=0.1)\n",
    "\n",
    "observations = np.zeros(len(df_features))\n",
    "mix_predictions = np.zeros(len(df_features))\n",
    "\n",
    "for train_index, val_index in kf.split(df_features):\n",
    "    \n",
    "    df_features_train = df_features.iloc[train_index]\n",
    "    df_features_val = df_features.iloc[val_index]\n",
    "    \n",
    "    df_label_train = df_features_train['target'].values\n",
    "    df_label_val = df_features_val['target'].values\n",
    "    \n",
    "    df_features_train.drop(labels=['target'], axis=1, inplace=True)\n",
    "    df_features_val.drop(labels=['target'], axis=1, inplace=True)\n",
    "    \n",
    "    model.fit(df_features_train, df_label_train, eval_set=[(df_features_val, df_label_val)],\n",
    "              early_stopping_rounds=5,\n",
    "              eval_metric='rmse',\n",
    "              verbose=True)\n",
    "    \n",
    "    observations[val_index] = df_label_val\n",
    "    mix_predictions[val_index] = model.predict(df_features_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "ax.plot(observations.reshape(-1, ), c='b', label='observations')\n",
    "ax.plot(mix_predictions.reshape(-1, ), c='r', label='hybrid')\n",
    "ax.plot(cnn_lstm_predictions.reshape(-1, ), c='g', label='cnn_lstm')\n",
    "ax.plot(prophet_predictions.reshape(-1, ), c='cyan', label='prophet')\n",
    "\n",
    "time = [str(d)[:10] for d in daily_data.iloc[-52 * 7:].index[::7].values]\n",
    "\n",
    "ax.set_title('Mix', fontsize=24)\n",
    "ax.set_xlim(0, 51)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "ax.set_xticklabels(labels=[0] + time[::4], rotation=45)\n",
    "ax.set_xlabel('Time', fontsize=24)\n",
    "ax.set_ylabel('A weekly consumption (100000)', fontsize=24)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(observations, mix_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
