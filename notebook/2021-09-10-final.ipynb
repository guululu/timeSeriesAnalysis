{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic structure of encoder - decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url='https://miro.medium.com/max/1400/1*w9hrrIEEIl5Uk3BiL4WFog.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic structure of a LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs42979-020-0101-1/MediaObjects/42979_2020_101_Fig1_HTML.png?as=webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://www.mcdonalds.com/is/image/content/dam/ch/nutrition/nfl-product/product/hero/t-mcdonalds-cheeseburger.jpg?$Product_Desktop$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://www.mcdonalds.com/is/image/content/dam/de/nutrition/items/hero/desktop/6163_thumb.jpg?$Product_Desktop$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder - Decoder 好處\n",
    "\n",
    "1. 減少計算量\n",
    "2. 客製化 Encoder- Decoder 結構\n",
    "3. 客製化 Encoder -> Decoder 的過程\n",
    "\n",
    "\n",
    "Reading: \n",
    "https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, RepeatVector, TimeDistributed, Flatten, LSTM, Input, Concatenate, Conv1D, Dropout, \\\n",
    "MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from IPython.display import Image\n",
    "# from IPython.core.display import HTML \n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from main.utils.data import load_data\n",
    "\n",
    "# prediction for four weeks\n",
    "n_out = 4\n",
    "# 7 weeks gap\n",
    "n_gap = 7\n",
    "# 28 days input\n",
    "n_input = 28\n",
    "\n",
    "\n",
    "def raw_data_preparation():\n",
    "    \n",
    "    df_parsed = load_data()\n",
    "\n",
    "    df = df_parsed.copy()\n",
    "    df = df.resample('1D').sum()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    df['week_day'] = [idx.weekday() for idx in df.index]\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['week_day'] == 0:\n",
    "            idx_start = idx\n",
    "            break  # 跳出 for loop\n",
    "\n",
    "    for idx, row in df.iloc[::-1].iterrows():  # 從後面數回來\n",
    "        if row['week_day'] == 6:\n",
    "            idx_end = idx\n",
    "            break\n",
    "    \n",
    "    return df.loc[idx_start: idx_end]\n",
    "\n",
    "\n",
    "def split_dataset(data):\n",
    "    \n",
    "    scale = 100000\n",
    "    \n",
    "    # split into standard weeks\n",
    "    train, test = data[:-50*7], data[-(50+n_gap+n_out)*7:]\n",
    "    y_train, y_test = train[:, 0]/scale, test[:, 0]/scale\n",
    "    \n",
    "    # data normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    train_norm = scaler.fit_transform(train)\n",
    "    test_norm = scaler.transform(test)\n",
    "    \n",
    "    # restructure into windows of weekly data\n",
    "    train_norm = np.array(np.split(train_norm, len(train_norm)/7))\n",
    "    test_norm = np.array(np.split(test_norm, len(test_norm)/7))\n",
    "    return train_norm, test_norm, y_train, y_test\n",
    "\n",
    "\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    \n",
    "    msle = mean_squared_log_error(actual, np.clip(predicted, 0, 100))\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    \n",
    "    return msle, mse\n",
    "            \n",
    "\n",
    "def to_supervised(train, train_label, n_input, n_out=6, n_gap=7):\n",
    "    \n",
    "    '''\n",
    "    n_input: days\n",
    "    n_out: measured in weeks\n",
    "    n_future: measured in weeks\n",
    "    '''\n",
    "    \n",
    "    # Multivariant input\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y, X_weekly = list(), list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data) - (n_out + n_gap) * 7):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_start = in_end + 7 * n_gap\n",
    "        out_end = out_start + 7 * n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            # Univariant version\n",
    "            '''\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "            x_input = x_input.reshape((len(x_input), 1))\n",
    "            '''\n",
    "            X.append(data[in_start:in_end, :])\n",
    "            y.append(np.array(np.split(train_label[out_start: out_end], n_out)).sum(axis=1))\n",
    "            X_weekly.append(np.array(np.split(data[in_start:in_end, :], n_out)).sum(axis=1))\n",
    "        # move along one time step\n",
    "        in_start += 7\n",
    "    return np.array(X), np.array(y), np.array(X_weekly)\n",
    "\n",
    "\n",
    "def forecast(model, history, n_input, n_out):\n",
    "    # flatten data\n",
    "    data = np.array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    \n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, :]\n",
    "    # reshape into [1, n_input, n_feature] Multivariant input\n",
    "    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "    \n",
    "    # weekly aggregated data\n",
    "    input_x_2 = data[-n_input:, :]\n",
    "    external = np.array(np.split(input_x_2, n_out)).sum(axis=1)\n",
    "    input_weekly = np.expand_dims(external, axis=0)\n",
    "    \n",
    "    # forecast the next week\n",
    "    yhat = model.predict([input_x, input_weekly], verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat\n",
    "                     \n",
    "                     \n",
    "def evaluation_model(train, test, label_train, label_test, n_input, n_out=6, n_gap=7):\n",
    "    \n",
    "    model = build_model(train, label_train, n_input=n_input, n_out=n_out, n_gap=n_gap)\n",
    "    history = [x for x in train[:-(n_out + n_gap)]]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in tqdm(range(len(test) - (n_out + n_gap))):\n",
    "        history.append(test[i, :])\n",
    "        yhat_sequence = forecast(model, history, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap) * 7: (i + n_out + n_gap) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff'] = df['A'].diff()\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff']]\n",
    "# daily_data = df[['A', 'C', 'G', 'A_diff', 'week', 'month', ...., 'holiday']]\n",
    "daily_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering data with four features: A, C, G, A_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 365 days base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "prediction_input = label_train[:-(n_out + n_gap) * 7]\n",
    "\n",
    "predictions = list()\n",
    "observations = list()\n",
    "\n",
    "for i in tqdm(range(len(test) - (n_out + n_gap))):\n",
    "    \n",
    "    prediction_input = np.concatenate((prediction_input, label_test[:7]))\n",
    "    \n",
    "    yhat = np.array([np.mean(prediction_input[-365:]) * 7] * 4)\n",
    "    y = label_test[(1 + n_gap) * 7: (1 + n_gap + n_out) * 7]  # in days \n",
    "    y = np.array(np.split(y, n_out)).sum(axis=1)\n",
    "    \n",
    "    predictions.append(yhat)\n",
    "    observations.append(y)\n",
    "    \n",
    "    label_test = label_test[7:]\n",
    "    \n",
    "predictions = np.array(predictions)\n",
    "oobservations = np.array(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - Decoder LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-019-55320-6/MediaObjects/41598_2019_55320_Fig3_HTML.png?as=webp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, train_label, n_input, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 35, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x, state_h, state_c = LSTM(64, activation='relu', return_state=True)(main_inputs)\n",
    "    \n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs')\n",
    "    y = LSTM(64, activation='relu', return_sequences=True)(weekly_inputs, initial_state=[state_h, state_c])\n",
    "    \n",
    "    y = TimeDistributed(Dense(12, activation='relu'))(y)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(y)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "lstm_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we simply use the last hidden state as inputs for the decoder?\n",
    "\n",
    "def build_model(train, train_label, n_input, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 30, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x, state_h, state_c = LSTM(64, activation='relu', return_state=True)(main_inputs)\n",
    "    \n",
    "    decoder_input = RepeatVector(n_out)(x)  # Repeatvector(n_out)(state_h)  \n",
    "    \n",
    "#     state_h = BatchNormalization(momentum=0.6)(state_h)\n",
    "#     state_c = BatchNormalization(momentum=0.6)(state_c)\n",
    "    \n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs') #無作用\n",
    "    y = LSTM(64, activation='elu', return_sequences=True, dropout=0.3, \n",
    "             recurrent_dropout=0.1)(decoder_input, initial_state=[state_h, state_c])\n",
    "    \n",
    "    y = TimeDistributed(Dense(12, activation='relu'))(y)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(y)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "lstm_rmse_v2 = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, train_label, n_input, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 25, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', # 加權平均\n",
    "               input_shape=(n_timesteps,n_features))(main_inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x, state_h, state_c = LSTM(64, activation='relu', return_state=True)(x)\n",
    "    \n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs')\n",
    "    x = LSTM(64, activation='relu', return_sequences=True)(weekly_inputs, initial_state=[state_h, state_c])\n",
    "    x = TimeDistributed(Dense(12, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(x)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "cnnlstm_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, train_label, n_input, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 25, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', \n",
    "               input_shape=(n_timesteps,n_features))(main_inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x, state_h, state_c = LSTM(64, activation='relu', return_state=True)(x)\n",
    "    decoder_input = RepeatVector(n_out)(x)\n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs')\n",
    "    \n",
    "    decoder_input = Concatenate(axis=2)([weekly_inputs, decoder_input])\n",
    "    \n",
    "    x = LSTM(64, activation='elu', return_sequences=True, dropout=0.3, \n",
    "             recurrent_dropout=0.1)(decoder_input, initial_state=[state_h, state_c])\n",
    "    x = TimeDistributed(Dense(12, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(x)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "# cnnlstm_rmse_v2 = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_rmse_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(np.arange(4), baseline_rmse, '-o', label='365 days prediction')\n",
    "ax.plot(np.arange(4), lstm_rmse, '-o', label='LSTM')\n",
    "ax.plot(np.arange(4), cnnlstm_rmse, '-o', label='CNN-LSTM')\n",
    "ax.plot(np.arange(4), lstm_rmse_v2, '-o', label='LSTM_v2')\n",
    "ax.plot(np.arange(4), cnnlstm_rmse_v2, '-o', label='CNN-LSTM_v2')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('RMSE (100k)')\n",
    "ax.set_xticks(np.arange(4))\n",
    "ax.set_xticklabels(np.arange(4) + 1)\n",
    "ax.set_xlabel(\"7 + n Week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "- feature extractor:\n",
    "    LSTM filter\n",
    "    TimeDistributed Dense filter\n",
    "\n",
    "- Adam optimizer:\n",
    "    learning rate: 0.001\n",
    "    beta_1: 0.9\n",
    "    beta_2: 0.999\n",
    "    epsilon: 1e-7\n",
    "    \n",
    "- training:\n",
    "    epochs (large epoch may result in overfitting)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# callbacks = [EarlyStopping(monitor='val_msle', patience=20, restore_best_weights=True),\n",
    "#              ModelCheckpoint(\"best_model.h5\", monitor=\"val_msle\", save_weights_only=False,\n",
    "#                              save_best_only=True, verbose=1),\n",
    "#              ReduceLROnPlateau(monitor='val_msle', patience=5, factor=0.9, min_lr=0.00001, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_lstm_model(train, test, label_train, label_test, n_input, lstm_filter, dense_filter_decoder,\n",
    "                          learning_rate, beta_1, beta_2, epsilon, epochs, n_out=6, n_gap=7):\n",
    "    \n",
    "    model = build_lstm_model(train, label_train, n_input=n_input, lstm_filter=int(lstm_filter), \n",
    "                            dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                            beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs), n_out=n_out, n_gap=n_gap)\n",
    "    history = [x for x in train[:-(n_out + n_gap)]]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in tqdm(range(len(test) - (n_out + n_gap))):\n",
    "        history.append(test[i, :])\n",
    "        yhat_sequence = forecast(model, history, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap) * 7: (i + n_out + n_gap) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations\n",
    "\n",
    "\n",
    "def build_lstm_model(train, train_label, n_input, lstm_filter, dense_filter_decoder,\n",
    "                     learning_rate, beta_1, beta_2, epsilon, epochs=35, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, batch_size = 0, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x, state_h, state_c = LSTM(lstm_filter, activation='relu', return_state=True)(main_inputs)\n",
    "    \n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs')\n",
    "    y = LSTM(lstm_filter, activation='relu', return_sequences=True)(weekly_inputs, initial_state=[state_h, state_c])\n",
    "    \n",
    "    y = TimeDistributed(Dense(dense_filter_decoder, activation='relu'))(y)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(y)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon,\n",
    "                     clipvalue=0.1)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['msle'])\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose, \n",
    "              shuffle=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bayesian Optimization\n",
    "\n",
    "def optimization_process(fn, pbounds: Dict) -> Dict:\n",
    "\n",
    "    \"\"\"\n",
    "    Bayesian optimization process interface. Returns hyperparameters of machine learning algorithms and the\n",
    "    corresponding out-of-fold (oof) predictions\n",
    "\n",
    "    Args:\n",
    "        fn: functional that will be optimized\n",
    "        pbounds: a dictionary having the boundary of parameters of fn\n",
    "\n",
    "    Returns:\n",
    "        A tuple of dictionary containing optimized hyperparameters and oof-predictions\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=fn,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1)\n",
    "\n",
    "    optimizer.maximize(init_points=8, n_iter=24)\n",
    "    \n",
    "    optimized_parameters = optimizer.max['params']\n",
    "\n",
    "    return optimized_parameters\n",
    "\n",
    "def lstm_training_process(epochs, lstm_filter, dense_filter_decoder,\n",
    "                          learning_rate, beta_1, beta_2, epsilon):\n",
    "    \n",
    "    \n",
    "    n_input = 28\n",
    "    n_out = 4\n",
    "    n_gap = 7\n",
    "    \n",
    "    train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "    \n",
    "    model = build_lstm_model(train, label_train, n_input=n_input, lstm_filter=int(lstm_filter), \n",
    "                            dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                            beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs), n_out=n_out, n_gap=n_gap)\n",
    "    history = [x for x in train[:-(n_out + n_gap)]]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in range(len(test) - (n_out + n_gap)):\n",
    "        history.append(test[i, :])\n",
    "        yhat_sequence = forecast(model, history, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap) * 7: (i + n_out + n_gap) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    rmse = mean_squared_error(observations, predictions)\n",
    "    \n",
    "    return -rmse\n",
    "\n",
    "# lstm_filter = 64\n",
    "# dense_filter_decoder = 12\n",
    "# learning_rate = 0.001\n",
    "# beta_1 = 0.9\n",
    "# beta_2 = 0.999\n",
    "# epsilon = 1e-7\n",
    "\n",
    "pbounds = {'epochs': (10, 40),\n",
    "           'lstm_filter': (48, 130),\n",
    "           'dense_filter_decoder': (8, 20),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.0000001, 0.00001)}\n",
    "\n",
    "optimized_parameters = optimization_process(lstm_training_process, pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization parameters:\n",
    "\n",
    "- beta_1: 0.5\n",
    "- beta_2: 0.7\n",
    "- dense_filter_decoder: 11\n",
    "- epochs: 17\n",
    "- epsilon: 1e-5\n",
    "- learning_rate: 0.0018176\n",
    "- lstm_filter: 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters = {'beta_1': 0.5,\n",
    "                        'beta_2': 0.7,\n",
    "                        'dense_filter_decoder': 11,\n",
    "                        'epochs': 17,\n",
    "                        'epsilon': 1e-5,\n",
    "                        'learning_rate': 0.0018176,\n",
    "                        'lstm_filter': 48}\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_lstm_model(train, test, label_train, label_test, n_input, n_out=4, n_gap=7, **optimized_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_lstm_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_lstm_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_cnn_lstm_model(train, test, label_train, label_test, n_input, attention_filter, lstm_filter, dense_filter_decoder,\n",
    "                              learning_rate, beta_1, beta_2, epsilon, epochs, n_out=6, n_gap=7):\n",
    "    \n",
    "    model = build_cnn_lstm_model(train, label_train, n_input=n_input, attention_filter=int(attention_filter), lstm_filter=int(lstm_filter), \n",
    "                                 dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                                 beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs), n_out=n_out, n_gap=n_gap)\n",
    "    history = [x for x in train[:-(n_out + n_gap)]]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in tqdm(range(len(test) - (n_out + n_gap))):\n",
    "        history.append(test[i, :])\n",
    "        yhat_sequence = forecast(model, history, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap) * 7: (i + n_out + n_gap) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations\n",
    "\n",
    "\n",
    "def build_cnn_lstm_model(train, train_label, n_input, attention_filter, lstm_filter, dense_filter_decoder,\n",
    "                         learning_rate, beta_1, beta_2, epsilon, epochs=35, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, batch_size = 0, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x = Conv1D(filters=attention_filter, kernel_size=3, activation='relu', padding='same', \n",
    "               input_shape=(n_timesteps,n_features))(main_inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x, state_h, state_c = LSTM(lstm_filter, activation='relu', return_state=True)(x)\n",
    "    \n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs')\n",
    "    x = LSTM(lstm_filter, activation='relu', return_sequences=True)(weekly_inputs, initial_state=[state_h, state_c])\n",
    "    x = TimeDistributed(Dense(dense_filter_decoder, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(x)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon,\n",
    "                     clipvalue=0.1)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['msle'])\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose, \n",
    "              shuffle=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm_training_process(epochs, attention_filter, lstm_filter, dense_filter_decoder,\n",
    "                              learning_rate, beta_1, beta_2, epsilon):\n",
    "    \n",
    "    \n",
    "    n_input = 28\n",
    "    n_out = 4\n",
    "    n_gap = 7\n",
    "    \n",
    "    train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "    \n",
    "    model = build_cnn_lstm_model(train, label_train, n_input=n_input, attention_filter=int(attention_filter), \n",
    "                                 lstm_filter=int(lstm_filter), dense_filter_decoder=int(dense_filter_decoder), \n",
    "                                 learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, \n",
    "                                 epochs=int(epochs), n_out=n_out, n_gap=n_gap)\n",
    "    history = [x for x in train[:-(n_out + n_gap)]]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in range(len(test) - (n_out + n_gap)):\n",
    "        history.append(test[i, :])\n",
    "        yhat_sequence = forecast(model, history, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap) * 7: (i + n_out + n_gap) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    rmse = mean_squared_error(observations, predictions)\n",
    "    \n",
    "    return -rmse\n",
    "\n",
    "# attention_filter = 64\n",
    "# lstm_filter = 64\n",
    "# dense_filter_decoder = 12\n",
    "# learning_rate = 0.001\n",
    "# beta_1 = 0.9\n",
    "# beta_2 = 0.999\n",
    "# epsilon = 1e-7\n",
    "\n",
    "pbounds = {'epochs': (10, 25),\n",
    "           'attention_filter': (48, 130),\n",
    "           'lstm_filter': (16, 130),\n",
    "           'dense_filter_decoder': (10, 40),\n",
    "           'learning_rate': (0.0001, 0.01),\n",
    "           'beta_1': (0.3, 0.95),\n",
    "           'beta_2': (0.3, 0.9999),\n",
    "           'epsilon': (0.0000001, 0.00001)}\n",
    "\n",
    "cnn_lstm_optimized_parameters = optimization_process(cnn_lstm_training_process, pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization parameters:\n",
    "\n",
    "- attention_filter: 95\n",
    "- beta_1: 0.3954\n",
    "- beta_2: 0.7125\n",
    "- dense_filter_decoder: 30\n",
    "- epochs: 11\n",
    "- epsilon: 4.19915e-06\n",
    "- learning_rate: 0.006974\n",
    "- lstm_filter: 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_optimized_parameters = {'attention_filter': 95,\n",
    "                        'beta_1': 0.3954,\n",
    "                        'beta_2': 0.7125,\n",
    "                        'dense_filter_decoder': 30,\n",
    "                        'epochs': 11,\n",
    "                        'epsilon': 4.19915e-06,\n",
    "                        'learning_rate': 0.006974,\n",
    "                        'lstm_filter': 63}\n",
    "\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_cnn_lstm_model(train, test, label_train, label_test, n_input, n_out=4, n_gap=7, **cnn_lstm_optimized_parameters)\n",
    "\n",
    "optimized_cnn_lstm_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_cnn_lstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(np.arange(4), baseline_rmse, '-o', label='365 days prediction')\n",
    "ax.plot(np.arange(4), lstm_rmse, '-o', label='LSTM')\n",
    "ax.plot(np.arange(4), cnnlstm_rmse, '-o', label='CNN-LSTM')\n",
    "ax.plot(np.arange(4), lstm_rmse_v2, '-o', label='LSTM_v2')\n",
    "ax.plot(np.arange(4), cnnlstm_rmse_v2, '-o', label='CNN-LSTM_v2')\n",
    "ax.plot(np.arange(4), optimized_lstm_rmse, '-o', label='LSTM, with fine tune')\n",
    "ax.plot(np.arange(4), optimized_cnn_lstm_rmse, '-o', label='CNN-LSTM, with fine tune')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('RMSE (100k)')\n",
    "ax.set_xticks(np.arange(4))\n",
    "ax.set_xticklabels(np.arange(4) + 1)\n",
    "ax.set_xlabel(\"7 + n Week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet\n",
    "\n",
    "- Dependence on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "scheduled_off = pd.to_datetime(['2018-02-15', '2018-02-16', '2018-02-17', '2018-02-18', '2018-12-30', '2018-12-31', '2019-01-01', \n",
    "                                '2019-02-02', '2019-02-03', '2019-02-04', '2019-02-05', '2019-02-06', '2019-02-07', '2019-02-08', \n",
    "                                '2019-02-09', '2019-02-10', '2019-12-30', '2019-12-31', '2020-01-23', '2020-01-24', '2020-01-25', \n",
    "                                '2020-01-26', '2020-01-27', '2021-01-01', '2021-02-11', '2021-02-12', '2021-02-13', '2021-02-14', \n",
    "                                '2019-04-05', '2019-04-06', '2019-04-07', '2016-02-06', '2016-02-07', '2016-02-08', '2016-02-09',\n",
    "                                '2016-02-10', '2016-04-04', '2016-09-16', '2016-12-29', '2016-12-30', '2016-12-31', '2017-01-27', \n",
    "                                '2017-01-28', '2017-01-29', '2017-01-30'])\n",
    "\n",
    "scheduled_off_df = pd.DataFrame({'holiday': 'scheduled_off',\n",
    "                                 'ds': scheduled_off,\n",
    "                                 'lower_window': -2,  # range of impact of off days\n",
    "                                 'upper_window': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to achieve the specification of Prophet\n",
    "\n",
    "df.index.name = 'ds'  # specify timestamp index name as ds\n",
    "df.reset_index(inplace=True)  # make index into column\n",
    "df.rename(columns={'A': 'y'}, inplace=True)  # rename the target column as y\n",
    "\n",
    "# train-test split\n",
    "\n",
    "df_train = df.iloc[:-350]\n",
    "df_test = df.iloc[-350:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet also has built-in yearly, weekly, and daily seasonality\n",
    "# Because we have only consumptions per day, we also turned off daily seasonality\n",
    "\n",
    "m = Prophet(holidays=scheduled_off_df, \n",
    "            growth='linear',\n",
    "            changepoint_prior_scale=0.005,\n",
    "            changepoint_range=0.9,\n",
    "            yearly_seasonality=10,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False)\n",
    "\n",
    "#  Add quarter seasonality by hand\n",
    "#  Give the name of your seasonality and the corresponding period\n",
    "m.add_seasonality(name='month', period=30, fourier_order=5) \n",
    "m.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduled_off_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=350)\n",
    "\n",
    "prediction = m.predict(future)\n",
    "fig = m.plot_components(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.plot(df_test['y'].values, label='Observation')\n",
    "ax.plot(prediction.iloc[-350:]['yhat'].values, label='Prophet prediction')\n",
    "\n",
    "ax.set_xticks(np.arange(0, 350, 50))\n",
    "ax.set_xticklabels([str(a)[:10] for a in df_test['ds'].values[0:350:50]])\n",
    "ax.set_title(\"Prophet Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://blog.floydhub.com/content/images/2018/12/queen-man-woman.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "月，週，日，季(1-3, 4-6, 7-9, 10-12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
