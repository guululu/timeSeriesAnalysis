{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from finta import TA\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, RepeatVector, TimeDistributed, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from main.utils.data import load_data\n",
    "\n",
    "df_parsed = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 4\n",
    "n_future = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the dataset into standard weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_parsed.copy()\n",
    "df = df.resample('1D').sum()\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['week_day'] = [idx.weekday() for idx in df.index]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['week_day'] == 0:\n",
    "        idx_start = idx\n",
    "        break\n",
    "        \n",
    "for idx, row in df.iloc[::-1].iterrows():\n",
    "    if row['week_day'] == 6:\n",
    "        idx_end = idx\n",
    "        break\n",
    "\n",
    "# Considering data with four features: A, C, G, A_diff\n",
    "\n",
    "df['A_diff'] = df['A'].diff()\n",
    "\n",
    "daily_data = df.loc[idx_start: idx_end][['A', 'C', 'G', 'A_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 100000\n",
    "\n",
    "def split_dataset(data):\n",
    "    \n",
    "    # split into standard weeks\n",
    "    train, test = data[7:-210], data[-210:]\n",
    "    y_train, y_test = train[:, 0]/scale, test[:, 0]/scale\n",
    "    \n",
    "    # data normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    train_norm = scaler.fit_transform(train)\n",
    "    test_norm = scaler.transform(test)\n",
    "    \n",
    "    # restructure into windows of weekly data\n",
    "    train_norm = np.array(np.split(train_norm, len(train_norm)/7))\n",
    "    test_norm = np.array(np.split(test_norm, len(test_norm)/7))\n",
    "    return train_norm, test_norm, y_train, y_test\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecasts(actual, predicted):\n",
    "    \n",
    "    msle = mean_squared_log_error(actual, np.clip(predicted, 0, 100))\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    \n",
    "    return msle, mse\n",
    "            \n",
    "\n",
    "def to_supervised(train, label_train, n_input, n_out=6, n_future=7):\n",
    "    \n",
    "    '''\n",
    "    n_input: days\n",
    "    n_out: measured in weeks\n",
    "    n_future: measured in weeks\n",
    "    '''\n",
    "    \n",
    "    # Multivariant input\n",
    "    # flatten data\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_start = in_end + 7 * n_future\n",
    "        out_end = out_start + 7 * n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            # Univariant version\n",
    "            '''\n",
    "            x_input = data[in_start:in_end, 0]\n",
    "             x_input = x_input.reshape((len(x_input), 1))\n",
    "            '''\n",
    "            X.append(data[in_start:in_end, :])\n",
    "            y.append(np.array(np.split(label_train[out_start: out_end], n_out)).sum(axis=1))\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def build_model(train, train_label, n_input, n_out=6, n_future=7):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, train_label, n_input, n_out=n_out, n_future=n_future) \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 20, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    \n",
    "    print(f\"n_timesteps = {n_timesteps}, n_features = {n_features}, n_outputs = {n_outputs}\")\n",
    "    \n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())  # reduce to a compact dense layer\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dropout(0.3)))\n",
    "    model.add(TimeDistributed(Dense(10, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))  # one for univariant output\n",
    "    model.compile(loss='msle', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n",
    "\n",
    "\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = np.array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, :]\n",
    "    # reshape into [1, n_input, n_feature] Multivariant input\n",
    "    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "    # forecast the next week\n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat\n",
    "                     \n",
    "                     \n",
    "def evaluation_model(train, test, label_train, label_test, n_input, n_out=6, n_future=7):\n",
    "    \n",
    "    model = build_model(train, label_train, n_input=n_input, n_out=n_out, n_future=n_future)\n",
    "    history = [x for x in train]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in tqdm(range(len(test) - (n_out + n_future) + 1)): # because we want to predict up to 13 weeks <-- wtf...\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_future) * 7: (i + n_out + n_future) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(test[i, :])\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps = 14, n_features = 4, n_outputs = 4\n",
      "Train on 995 samples\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 4s 4ms/sample - loss: 1.2286\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6276\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6173\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6006\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5988\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6020\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6032\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6023\n",
      "Epoch 9/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5913\n",
      "Epoch 10/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5886\n",
      "Epoch 11/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5872\n",
      "Epoch 12/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5777\n",
      "Epoch 13/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5771\n",
      "Epoch 14/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5736\n",
      "Epoch 15/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.5640\n",
      "Epoch 16/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.5643\n",
      "Epoch 17/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5512\n",
      "Epoch 18/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.5452\n",
      "Epoch 19/20\n",
      "995/995 [==============================] - 1s 2ms/sample - loss: 0.5315\n",
      "Epoch 20/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSE = 0.6158368968175252, MSE = 19.202145328549598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=14,\n",
    "                                             n_out=n_out, n_future=n_future)\n",
    "\n",
    "mlse, mse = evaluate_forecasts(observations, predictions)\n",
    "\n",
    "print(f\"MLSE = {mlse}, MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take holiday into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "known_holidays = [pd.Timestamp('2018-02-15'), pd.Timestamp('2018-02-16'), pd.Timestamp('2018-02-17'), pd.Timestamp('2018-02-18'),\n",
    "                  pd.Timestamp('2018-12-30'), pd.Timestamp('2018-12-31'), pd.Timestamp('2019-01-01'), pd.Timestamp('2019-02-02'),\n",
    "                  pd.Timestamp('2019-02-03'), pd.Timestamp('2019-02-04'), pd.Timestamp('2019-02-05'), pd.Timestamp('2019-02-06'),\n",
    "                  pd.Timestamp('2019-02-07'), pd.Timestamp('2019-02-08'), pd.Timestamp('2019-02-09'), pd.Timestamp('2019-02-10'),\n",
    "                  pd.Timestamp('2019-12-30'), pd.Timestamp('2019-12-31'), pd.Timestamp('2020-01-23'), pd.Timestamp('2020-01-24'),\n",
    "                  pd.Timestamp('2020-01-25'), pd.Timestamp('2020-01-26'), pd.Timestamp('2020-01-27'), pd.Timestamp('2021-01-01'),\n",
    "                  pd.Timestamp('2021-02-11'), pd.Timestamp('2021-02-12'), pd.Timestamp('2021-02-13'), pd.Timestamp('2021-02-14'),\n",
    "                  pd.Timestamp('2019-04-05'), pd.Timestamp('2019-04-06'), pd.Timestamp('2019-04-07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_parsed.copy()\n",
    "df = df.resample('1D').sum()\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['week_day'] = [idx.weekday() for idx in df.index]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['week_day'] == 0:\n",
    "        idx_start = idx\n",
    "        break\n",
    "        \n",
    "for idx, row in df.iloc[::-1].iterrows():\n",
    "    if row['week_day'] == 6:\n",
    "        idx_end = idx\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['off'] = df.index.isin(known_holidays).astype(int)\n",
    "df['off_shift'] = df['off'].shift(-56)  # 讓數據看見放假\n",
    "\n",
    "df['A_diff'] = df['A'].diff()\n",
    "\n",
    "daily_data = df.loc[idx_start: idx_end][['A', 'C', 'G', 'A_diff', 'off_shift']]\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps = 14, n_features = 5, n_outputs = 4\n",
      "Train on 995 samples\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 4s 4ms/sample - loss: 1.2282\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6174\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.6069\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5880\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5818\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5802\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5746\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5642\n",
      "Epoch 9/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5476\n",
      "Epoch 10/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5430\n",
      "Epoch 11/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5378\n",
      "Epoch 12/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5290\n",
      "Epoch 13/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5243\n",
      "Epoch 14/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5188\n",
      "Epoch 15/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5055\n",
      "Epoch 16/20\n",
      "995/995 [==============================] - 1s 2ms/sample - loss: 0.5096\n",
      "Epoch 17/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4850\n",
      "Epoch 18/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4817\n",
      "Epoch 19/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4706\n",
      "Epoch 20/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.38it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=14, n_out=n_out, n_future=n_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSE = 0.6419736790962692, MSE = 18.240308873138318\n"
     ]
    }
   ],
   "source": [
    "mlse, mse = evaluate_forecasts(observations, predictions)\n",
    "\n",
    "print(f\"MLSE = {mlse}, MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take holiday into consideration\n",
    "\n",
    "外部數據 台灣塑膠工業 1301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_data(code: int):\n",
    "    \n",
    "    # yahoo finance 下載股票\n",
    "    \n",
    "    data = yf.Ticker(f'{code}.TW')\n",
    "\n",
    "    df = data.history(start='2017-01-01', end='2021-07-31')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps = 14, n_features = 7, n_outputs = 4\n",
      "Train on 995 samples\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 4s 4ms/sample - loss: 1.1510\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5951\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5702\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5513\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5435\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5461\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5507\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5422\n",
      "Epoch 9/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5257\n",
      "Epoch 10/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5187\n",
      "Epoch 11/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5210\n",
      "Epoch 12/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5083\n",
      "Epoch 13/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.5092\n",
      "Epoch 14/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.4983\n",
      "Epoch 15/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.4875\n",
      "Epoch 16/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.4801\n",
      "Epoch 17/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4714\n",
      "Epoch 18/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4656\n",
      "Epoch 19/20\n",
      "995/995 [==============================] - 2s 2ms/sample - loss: 0.4552\n",
      "Epoch 20/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSE = 0.553225648028478, MSE = 17.510616321541335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_parsed.copy()\n",
    "df = df.resample('1D').sum()\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['week_day'] = [idx.weekday() for idx in df.index]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['week_day'] == 0:\n",
    "        idx_start = idx\n",
    "        break\n",
    "        \n",
    "for idx, row in df.iloc[::-1].iterrows():\n",
    "    if row['week_day'] == 6:\n",
    "        idx_end = idx\n",
    "        break\n",
    "\n",
    "df['off'] = df.index.isin(known_holidays).astype(int)\n",
    "df['off_shift'] = df['off'].shift(-7*(n_out + n_future))  # 讓數據看見放假\n",
    "df['A_diff'] = df['A'].diff()\n",
    "df['A_ewm_std'] = df['A'].ewm(span=14).std()\n",
    "\n",
    "df_formosa = load_stock_data(1301)\n",
    "\n",
    "df_formosa_macd = TA.MACD(df_formosa).rename(columns={'MACD': \"FORMOSA_MACD\", 'SIGNAL': \"FORMOSA_SIGNAL\"})\n",
    "df_formosa_vbm = TA.VBM(df_formosa).to_frame().rename(columns={'VBM': \"FORMOSA_VBM\"})\n",
    "df_formosa_ewm = TA.EVWMA(df_formosa, period=5).to_frame().rename(columns={'5 period EVWMA.': 'FORMOSA_EVWMA'})\n",
    "\n",
    "df = df.join([df_formosa_macd, df_formosa_vbm, df_formosa_ewm]).fillna(method='bfill')\n",
    "\n",
    "columns = ['A', 'C', 'G', 'A_diff', 'FORMOSA_SIGNAL', 'A_ewm_std', 'off_shift']\n",
    "\n",
    "daily_data = df.loc[idx_start: idx_end][columns]\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=14, n_out=n_out, n_future=n_future)\n",
    "\n",
    "mlse, mse = evaluate_forecasts(observations, predictions)\n",
    "\n",
    "print(f\"MLSE = {mlse}, MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take holiday into consideration\n",
    "\n",
    "外部數據:\n",
    "* 台灣塑膠工業 1301\n",
    "* 榮運        2607 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_feature(code):\n",
    "    \n",
    "    df = load_stock_data(code)\n",
    "    df_macd = TA.MACD(df).rename(columns={'MACD': f\"{code}_MACD\", 'SIGNAL': f\"{code}_SIGNAL\"})\n",
    "    df_vbm = TA.VBM(df).to_frame().rename(columns={'VBM': f\"{code}_VBM\"})\n",
    "    df_ewm = TA.EVWMA(df, period=5).to_frame().rename(columns={'5 period EVWMA.': f'{code}_EVWMA'})\n",
    "    \n",
    "    return pd.concat([df_macd, df_vbm, df_ewm], axis=1).fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps = 14, n_features = 8, n_outputs = 4\n",
      "Train on 995 samples\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 4s 4ms/sample - loss: 1.0677\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5994\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5784\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5548\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5477\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5533\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5566\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5432\n",
      "Epoch 9/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5256\n",
      "Epoch 10/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5245\n",
      "Epoch 11/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5195\n",
      "Epoch 12/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5046\n",
      "Epoch 13/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5020\n",
      "Epoch 14/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4922\n",
      "Epoch 15/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4868\n",
      "Epoch 16/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4628\n",
      "Epoch 17/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4690\n",
      "Epoch 18/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4443\n",
      "Epoch 19/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4373\n",
      "Epoch 20/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSE = 1.4116147792754998, MSE = 117.08612166979623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_parsed.copy()\n",
    "df = df.resample('1D').sum()\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['week_day'] = [idx.weekday() for idx in df.index]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['week_day'] == 0:\n",
    "        idx_start = idx\n",
    "        break\n",
    "        \n",
    "for idx, row in df.iloc[::-1].iterrows():\n",
    "    if row['week_day'] == 6:\n",
    "        idx_end = idx\n",
    "        break\n",
    "\n",
    "df['off'] = df.index.isin(known_holidays).astype(int)\n",
    "df['off_shift'] = df['off'].shift(-7*(n_out + n_future))  # 讓數據看見放假\n",
    "df['A_diff'] = df['A'].diff()\n",
    "df['A_ewm_std'] = df['A'].ewm(span=14).std()\n",
    "\n",
    "df_1301 = stock_feature(1301)\n",
    "df_2607 = stock_feature(2607)\n",
    "\n",
    "df = df.join([df_1301, df_2607]).fillna(method='bfill')\n",
    "\n",
    "columns = ['A', 'C', 'G', 'A_diff', '1301_SIGNAL', 'A_ewm_std', 'off_shift', '2607_SIGNAL']\n",
    "\n",
    "daily_data = df.loc[idx_start: idx_end][columns]\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=14, n_out=n_out, n_future=n_future)\n",
    "\n",
    "mlse, mse = evaluate_forecasts(observations, predictions)\n",
    "\n",
    "print(f\"MLSE = {mlse}, MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion COVID19 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19 = pd.read_csv('data/owid-covid-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_USA = covid19[covid19['iso_code']=='USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-22'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid19_USA['date'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = [pd.Timestamp(covid19_USA['date'].iloc[i]) for i in range(len(covid19_USA))]\n",
    "\n",
    "columns = ['total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths', 'new_deaths_smoothed', 'icu_patients',\n",
    "           'hosp_patients']\n",
    "\n",
    "data = covid19_USA[columns].values\n",
    "\n",
    "df_covid_USA = pd.DataFrame(data=data, columns=columns, index=date)\n",
    "\n",
    "df_covid_USA['week_day'] = [idx.weekday() for idx in df_covid_USA.index]\n",
    "df_covid_USA['new_cases_7_sum'] = df_covid_USA['new_cases'].rolling(7, min_periods=1).sum()\n",
    "df_covid_USA['new_deaths_7_sum'] = df_covid_USA['new_deaths'].rolling(7, min_periods=1).sum()\n",
    "\n",
    "for idx, row in df_covid_USA.iterrows():\n",
    "    if row['week_day'] == 0:\n",
    "        idx_start = idx\n",
    "        break\n",
    "        \n",
    "for idx, row in df_covid_USA.iloc[::-1].iterrows():\n",
    "    if row['week_day'] == 6:\n",
    "        idx_end = idx\n",
    "        break\n",
    "        \n",
    "df_covid_USA = df_covid_USA.loc[idx_start: idx_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manufacturer data\n",
    "\n",
    "df = df_parsed.copy()\n",
    "df = df.resample('1D').sum()\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['week_day'] = [idx.weekday() for idx in df.index]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['week_day'] == 0:\n",
    "        idx_start = idx\n",
    "        break\n",
    "        \n",
    "for idx, row in df.iloc[::-1].iterrows():\n",
    "    if row['week_day'] == 6:\n",
    "        idx_end = idx\n",
    "        break\n",
    "\n",
    "df['off'] = df.index.isin(known_holidays).astype(int)\n",
    "df['off_shift'] = df['off'].shift(-7*(n_out + n_future))  # 讓數據看見放假\n",
    "df['A_diff'] = df['A'].diff()\n",
    "df['A_ewm_std'] = df['A'].ewm(span=14).std()\n",
    "\n",
    "columns = ['total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths', 'new_deaths_smoothed', 'icu_patients',\n",
    "           'hosp_patients', 'new_cases_7_sum', 'new_deaths_7_sum']\n",
    "\n",
    "df = df.join([df_covid_USA[columns]]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>...</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>icu_patients</th>\n",
       "      <th>hosp_patients</th>\n",
       "      <th>new_cases_7_sum</th>\n",
       "      <th>new_deaths_7_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易日期</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>6500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>126000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>123500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>514000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>199700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>51000.0</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>251700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40600.0</td>\n",
       "      <td>43000.0</td>\n",
       "      <td>239500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>189000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>320800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>201000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401330.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>88800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>199200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>309500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234250.0</td>\n",
       "      <td>21500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>36900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>328100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70600.0</td>\n",
       "      <td>50700.0</td>\n",
       "      <td>168600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A        B         C    D         E        F         G  \\\n",
       "交易日期                                                                        \n",
       "2018-01-01    6500.0      0.0   96250.0  0.0   79700.0      0.0  106700.0   \n",
       "2018-01-02  126000.0      0.0  246750.0  0.0   14300.0      0.0  231100.0   \n",
       "2018-01-03  123500.0      0.0  514000.0  0.0       0.0      0.0  209500.0   \n",
       "2018-01-04  199700.0      0.0  140100.0  0.0  109900.0      0.0  302800.0   \n",
       "2018-01-05   51000.0  63000.0  251700.0  0.0   40600.0  43000.0  239500.0   \n",
       "2018-01-06  189000.0      0.0  337400.0  0.0   21500.0      0.0  320800.0   \n",
       "2018-01-07  201000.0      0.0  401330.0  0.0       0.0      0.0  286300.0   \n",
       "2018-01-08   88800.0      0.0  250200.0  0.0   27450.0      0.0   14250.0   \n",
       "2018-01-09  199200.0      0.0  309500.0  0.0   53600.0      0.0  234250.0   \n",
       "2018-01-10   36900.0      0.0  328100.0  0.0   70600.0  50700.0  168600.0   \n",
       "\n",
       "                  H        I    J  ...  total_cases  new_cases  \\\n",
       "交易日期                               ...                           \n",
       "2018-01-01      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-02      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-03      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-04      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-05      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-06      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-07      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-08      0.0  76000.0  0.0  ...          0.0        0.0   \n",
       "2018-01-09  21500.0      0.0  0.0  ...          0.0        0.0   \n",
       "2018-01-10      0.0      0.0  0.0  ...          0.0        0.0   \n",
       "\n",
       "            new_cases_smoothed  total_deaths  new_deaths  new_deaths_smoothed  \\\n",
       "交易日期                                                                            \n",
       "2018-01-01                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-02                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-03                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-04                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-05                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-06                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-07                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-08                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-09                 0.0           0.0         0.0                  0.0   \n",
       "2018-01-10                 0.0           0.0         0.0                  0.0   \n",
       "\n",
       "            icu_patients  hosp_patients  new_cases_7_sum  new_deaths_7_sum  \n",
       "交易日期                                                                        \n",
       "2018-01-01           0.0            0.0              0.0               0.0  \n",
       "2018-01-02           0.0            0.0              0.0               0.0  \n",
       "2018-01-03           0.0            0.0              0.0               0.0  \n",
       "2018-01-04           0.0            0.0              0.0               0.0  \n",
       "2018-01-05           0.0            0.0              0.0               0.0  \n",
       "2018-01-06           0.0            0.0              0.0               0.0  \n",
       "2018-01-07           0.0            0.0              0.0               0.0  \n",
       "2018-01-08           0.0            0.0              0.0               0.0  \n",
       "2018-01-09           0.0            0.0              0.0               0.0  \n",
       "2018-01-10           0.0            0.0              0.0               0.0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps = 14, n_features = 10, n_outputs = 4\n",
      "Train on 995 samples\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 4s 4ms/sample - loss: 1.1341\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5955\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5753\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5577\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5511\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5510\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5449\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5454\n",
      "Epoch 9/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5270\n",
      "Epoch 10/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5208\n",
      "Epoch 11/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5126\n",
      "Epoch 12/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5006\n",
      "Epoch 13/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5081\n",
      "Epoch 14/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4948\n",
      "Epoch 15/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4777\n",
      "Epoch 16/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4785\n",
      "Epoch 17/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4701\n",
      "Epoch 18/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4664\n",
      "Epoch 19/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4545\n",
      "Epoch 20/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSE = 0.4986503253579981, MSE = 14.147173289981186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_1301 = stock_feature(1301)\n",
    "# df_2607 = stock_feature(2607)\n",
    "\n",
    "df = df.join([df_1301]).fillna(method='bfill')\n",
    "\n",
    "columns = ['A', 'C', 'G', 'A_diff', '1301_SIGNAL', 'A_ewm_std', 'off_shift',\n",
    "           'new_cases_smoothed', 'new_deaths_smoothed', 'new_cases_7_sum']\n",
    "\n",
    "daily_data = df.loc[idx_start: idx_end][columns]\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=14, n_out=n_out, n_future=n_future)\n",
    "\n",
    "mlse, mse = evaluate_forecasts(observations, predictions)\n",
    "\n",
    "print(f\"MLSE = {mlse}, MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Taiwan into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covid_country(df, iso_code):\n",
    "    \n",
    "    covid19 = df[df['iso_code']==iso_code]\n",
    "    \n",
    "    date = [pd.Timestamp(covid19['date'].iloc[i]) for i in range(len(covid19))]\n",
    "\n",
    "    columns = ['total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths', 'new_deaths_smoothed', 'icu_patients',\n",
    "               'hosp_patients']\n",
    "\n",
    "    data = covid19[columns].values\n",
    "    \n",
    "    columns = [f'{iso_code}_{c}' for c in columns]\n",
    "    \n",
    "    df_covid = pd.DataFrame(data=data, columns=columns, index=date)\n",
    "\n",
    "    df_covid['week_day'] = [idx.weekday() for idx in df_covid.index]\n",
    "    df_covid[f'{iso_code}_new_cases_7_sum'] = df_covid[f'{iso_code}_new_cases'].rolling(7, min_periods=1).sum()\n",
    "    df_covid[f'{iso_code}_new_deaths_7_sum'] = df_covid[f'{iso_code}_new_deaths'].rolling(7, min_periods=1).sum()\n",
    "    df_covid[f'{iso_code}_new_cases_3_sum'] = df_covid[f'{iso_code}_new_cases'].rolling(3, min_periods=1).sum()\n",
    "    df_covid[f'{iso_code}_new_deaths_3_sum'] = df_covid[f'{iso_code}_new_deaths'].rolling(3, min_periods=1).sum()\n",
    "    \n",
    "    for idx, row in df_covid.iterrows():\n",
    "        if row['week_day'] == 0:\n",
    "            idx_start = idx\n",
    "            break\n",
    "\n",
    "    for idx, row in df_covid.iloc[::-1].iterrows():\n",
    "        if row['week_day'] == 6:\n",
    "            idx_end = idx\n",
    "            break\n",
    "\n",
    "    df_covid = df_covid.loc[idx_start: idx_end]\n",
    "    \n",
    "    df_covid.drop(labels=['week_day'], axis=1, inplace=True)\n",
    "    \n",
    "    return df_covid.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TW = covid_country(covid19, \"TWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US = covid_country(covid19, \"USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_parsed.copy()\n",
    "df = df.resample('1D').sum()\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df['week_day'] = [idx.weekday() for idx in df.index]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['week_day'] == 0:\n",
    "        idx_start = idx\n",
    "        break\n",
    "        \n",
    "for idx, row in df.iloc[::-1].iterrows():\n",
    "    if row['week_day'] == 6:\n",
    "        idx_end = idx\n",
    "        break\n",
    "\n",
    "df['off'] = df.index.isin(known_holidays).astype(int)\n",
    "df['off_shift'] = df['off'].shift(-7*(n_out + n_future))  # 讓數據看見放假\n",
    "df['A_diff'] = df['A'].diff()\n",
    "df['A_ewm_std'] = df['A'].ewm(span=14).std()\n",
    "\n",
    "df = df.join([df_US, df_TW]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
       "       'O', 'P', 'week_day', 'off', 'off_shift', 'A_diff', 'A_ewm_std',\n",
       "       'USA_total_cases', 'USA_new_cases', 'USA_new_cases_smoothed',\n",
       "       'USA_total_deaths', 'USA_new_deaths', 'USA_new_deaths_smoothed',\n",
       "       'USA_icu_patients', 'USA_hosp_patients', 'USA_new_cases_7_sum',\n",
       "       'USA_new_deaths_7_sum', 'USA_new_cases_3_sum', 'USA_new_deaths_3_sum',\n",
       "       'TWN_total_cases', 'TWN_new_cases', 'TWN_new_cases_smoothed',\n",
       "       'TWN_total_deaths', 'TWN_new_deaths', 'TWN_new_deaths_smoothed',\n",
       "       'TWN_icu_patients', 'TWN_hosp_patients', 'TWN_new_cases_7_sum',\n",
       "       'TWN_new_deaths_7_sum', 'TWN_new_cases_3_sum', 'TWN_new_deaths_3_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps = 14, n_features = 11, n_outputs = 4\n",
      "Train on 995 samples\n",
      "Epoch 1/20\n",
      "995/995 [==============================] - 3s 3ms/sample - loss: 1.1203\n",
      "Epoch 2/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5929\n",
      "Epoch 3/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5660\n",
      "Epoch 4/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5520\n",
      "Epoch 5/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5450\n",
      "Epoch 6/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5445\n",
      "Epoch 7/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5420\n",
      "Epoch 8/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5412\n",
      "Epoch 9/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5201\n",
      "Epoch 10/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5158\n",
      "Epoch 11/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5091\n",
      "Epoch 12/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5000\n",
      "Epoch 13/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.5025\n",
      "Epoch 14/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4931\n",
      "Epoch 15/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4663\n",
      "Epoch 16/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4704\n",
      "Epoch 17/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4604\n",
      "Epoch 18/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4448\n",
      "Epoch 19/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4331\n",
      "Epoch 20/20\n",
      "995/995 [==============================] - 1s 1ms/sample - loss: 0.4337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLSE = 0.48959520874743706, MSE = 14.862078764089658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_1301 = stock_feature(1301)\n",
    "# df_2607 = stock_feature(2607)\n",
    "\n",
    "df = df.join([df_1301]).fillna(method='bfill')\n",
    "\n",
    "columns = ['A', 'C', 'G', 'A_diff', '1301_SIGNAL', 'A_ewm_std', 'off_shift',\n",
    "           'USA_new_cases_smoothed', 'USA_new_deaths_smoothed', 'TWN_new_cases_smoothed',\n",
    "           'TWN_new_deaths_smoothed']\n",
    "\n",
    "daily_data = df.loc[idx_start: idx_end][columns]\n",
    "\n",
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=14, n_out=n_out, n_future=n_future)\n",
    "\n",
    "mlse, mse = evaluate_forecasts(observations, predictions)\n",
    "\n",
    "print(f\"MLSE = {mlse}, MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
