{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal ARIMA (SARIMA)\n",
    "\n",
    "Box - Jenkins Seasonal ARIMA model\n",
    "\n",
    "\\* Data might contain seasonal periodic component in addition to correlation with recent lags.\n",
    "\n",
    "\\* It repeats every $s$ observations.\n",
    "\n",
    "\\* For a time series of monthly observations, $X_t$ might depend on annual lags. i.e. $X_{t-12}$, $X_{t-24}$ \n",
    "\n",
    "\\* Quarterly data might have period of $s=4$\n",
    "\n",
    "## Seasonal ARIMA process (SARIMA)\n",
    "\n",
    "$SARIMA(p,d,q, P,D,Q)_s$ has the form\n",
    "\n",
    "$$\\Phi_P(B^s)\\phi_p(B)(1-B^s)^D(1-B)^dX_t = \\Theta_Q(B^s)\\theta_q(B)Z_t$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\theta_q(B) = 1 + \\theta_1B + \\cdots +\\theta_qB^q$$\n",
    "\n",
    "$$\\Theta_Q(B^s) = 1 + \\Theta_1B^s + \\Theta_2B^{2s} + \\cdots + \\Theta_QB^{Qs}$$\n",
    "\n",
    "$$\\phi_p(B) = 1 - \\phi_1B - \\phi_2B^2 - \\cdots - \\phi_pB^p$$\n",
    "\n",
    "$$\\Phi_P(B^s) = 1 - \\Phi_1B^s - \\Phi_2B^{2s} - \\cdots - \\Phi_PB^{Ps}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.path.join(os.getcwd(), 'data', 'raw')\n",
    "os.path.isdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.path.join(os.getcwd(), 'data', 'raw', \"原始資料.xlsx\")\n",
    "os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from main.utils.data import split_dataset, raw_data_preparation\n",
    "\n",
    "df = raw_data_preparation()\n",
    "df['week'] = [idx.week for idx in df.index]\n",
    "df['year'] = [idx.year for idx in df.index]\n",
    "\n",
    "df_A_year_week = df.groupby(['week', 'year'], as_index=False)['A'].sum()\n",
    "\n",
    "df_A_year_week.sort_values(by=['year', 'week'], inplace=True)\n",
    "\n",
    "df_A_year_week.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_A_year_week = df_A_year_week[['A']].values\n",
    "\n",
    "week = 50\n",
    "n_gap = 7\n",
    "n_out = 4\n",
    "scale = 100000\n",
    "\n",
    "train, test = df_A_year_week[:-week], df_A_year_week[-(week + n_gap + n_out):]\n",
    "y_train, y_test = train[:, 0] / scale, test[:, 0] / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P=1, s=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "history = list(copy(y_train[-(n_gap+n_out):]))\n",
    "future = copy(y_test)\n",
    "\n",
    "predictions = []\n",
    "observations = []\n",
    "\n",
    "for i in range(len(y_test) - (n_out + n_gap)):\n",
    "    history.append(future[i])\n",
    "    \n",
    "    sarima_p1 = SARIMAX(history, order=(0,0,0), seasonal_order=(1,0,0,11))\n",
    "    model_p1 = sarima_p1.fit()\n",
    "    yhat_sequence = model_p1.forecast(steps=11)[-4:]\n",
    "    predictions.append(yhat_sequence)\n",
    "    observations.append(future[i + 1 + n_gap: i + 1 + n_gap + n_out])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "observations = np.array(observations)\n",
    "\n",
    "sarima_rmse_p1 = np.sqrt(np.square(predictions-observations).mean(axis=0))\n",
    "\n",
    "print(sarima_rmse_p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q=1, s=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(copy(y_train[-(n_gap+n_out):]))\n",
    "future = copy(y_test)\n",
    "\n",
    "predictions = []\n",
    "observations = []\n",
    "\n",
    "for i in range(len(y_test) - (n_out + n_gap)):\n",
    "    history.append(future[i])\n",
    "    \n",
    "    sarima_q1 = SARIMAX(history, order=(0,0,0), seasonal_order=(0,0,1,11))\n",
    "    model_q1 = sarima_q1.fit()\n",
    "    yhat_sequence = model_q1.forecast(steps=11)[-4:]\n",
    "    predictions.append(yhat_sequence)\n",
    "    observations.append(future[i + 1 + n_gap: i + 1 + n_gap + n_out])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "observations = np.array(observations)\n",
    "\n",
    "sarima_rmse_q1 = np.sqrt(np.square(predictions-observations).mean(axis=0))\n",
    "\n",
    "print(sarima_rmse_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P=1, Q=1, s=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(copy(y_train[-(n_gap+n_out):]))\n",
    "future = copy(y_test)\n",
    "\n",
    "predictions = []\n",
    "observations = []\n",
    "\n",
    "for i in range(len(y_test) - (n_out + n_gap)):\n",
    "    history.append(future[i])\n",
    "    \n",
    "    sarima_p1_q1 = SARIMAX(history, order=(0,0,0), seasonal_order=(1,0,1,11))\n",
    "    model_p1_q1 = sarima_p1_q1.fit()\n",
    "    yhat_sequence = model_p1_q1.forecast(steps=11)[-4:]\n",
    "    predictions.append(yhat_sequence)\n",
    "    observations.append(future[i + 1 + n_gap: i + 1 + n_gap + n_out])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "observations = np.array(observations)\n",
    "\n",
    "sarima_rmse_p1_q1 = np.sqrt(np.square(predictions-observations).mean(axis=0))\n",
    "\n",
    "print(sarima_rmse_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P=1, s=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(copy(y_train[-(n_gap+n_out):]))\n",
    "future = copy(y_test)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "observations = []\n",
    "\n",
    "for i in range(len(y_test) - (n_out + n_gap)):\n",
    "    history.append(future[i])\n",
    "    \n",
    "    sarima_p1 = SARIMAX(history, order=(0,0,0), seasonal_order=(1,0,0,8))\n",
    "    model_p1 = sarima_p1.fit()\n",
    "    yhat_sequence = model_p1.forecast(steps=11)[-4:]\n",
    "    predictions.append(yhat_sequence)\n",
    "    observations.append(future[i + 1 + n_gap: i + 1 + n_gap + n_out])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "observations = np.array(observations)\n",
    "\n",
    "sarima_rmse_p1 = np.sqrt(np.square(predictions-observations).mean(axis=0))\n",
    "\n",
    "print(sarima_rmse_p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P=1, s=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(copy(y_train[-(n_gap+n_out):]))\n",
    "future = copy(y_test)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "observations = []\n",
    "\n",
    "for i in range(len(y_test) - (n_out + n_gap)):\n",
    "    history.append(future[i])\n",
    "    \n",
    "    sarima_p1 = SARIMAX(history, order=(0,0,0), seasonal_order=(1,0,0,4))\n",
    "    model_p1 = sarima_p1.fit()\n",
    "    yhat_sequence = model_p1.forecast(steps=11)[-4:]\n",
    "    predictions.append(yhat_sequence)\n",
    "    observations.append(future[i + 1 + n_gap: i + 1 + n_gap + n_out])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "observations = np.array(observations)\n",
    "\n",
    "sarima_rmse_p1 = np.sqrt(np.square(predictions-observations).mean(axis=0))\n",
    "\n",
    "print(sarima_rmse_p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder LSTM + Time \n",
    "\n",
    "We consider a simple LSTM with time info as categorical inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, RepeatVector, TimeDistributed, Flatten, LSTM, Input, Concatenate, Conv1D, Dropout, \\\n",
    "MaxPooling1D, BatchNormalization, Embedding, Add, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "\n",
    "from main.utils.data import split_dataset, raw_data_preparation\n",
    "from main.utils.utils import to_supervised, optimization_process\n",
    "from main.model.LSTM_model import build_lstm_v1_model, build_lstm_v2_model, build_lstm_v3_model\n",
    "\n",
    "# prediction for four weeks\n",
    "n_out = 4\n",
    "# 7 weeks gap\n",
    "n_gap = 7\n",
    "# 28 days input\n",
    "n_input = 28\n",
    "\n",
    "\n",
    "def forecast(model, history, n_input, n_out):\n",
    "    # flatten data\n",
    "    data = np.array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    \n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, :]\n",
    "    # reshape into [1, n_input, n_feature] Multivariant input\n",
    "    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "    \n",
    "    # weekly aggregated data\n",
    "    input_x_2 = data[-n_input:, :]\n",
    "    external = np.array(np.split(input_x_2, n_out)).sum(axis=1)\n",
    "    input_weekly = np.expand_dims(external, axis=0)\n",
    "    \n",
    "    # forecast the next week\n",
    "    yhat = model.predict([input_x, input_weekly], verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat\n",
    "                     \n",
    "                     \n",
    "def evaluation_model(model, train, test, label_train, label_test, n_input, n_out=6, n_gap=7):\n",
    "    \n",
    "#     model = build_model(train, label_train, n_input=n_input, n_out=n_out, n_gap=n_gap)\n",
    "    history = [x for x in train[:-(n_out + n_gap)]]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in tqdm(range(len(test) - (n_out + n_gap))):\n",
    "        history.append(test[i, :])\n",
    "        yhat_sequence = forecast(model, history, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap + 1) * 7: (i + n_out + n_gap + 1) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - Decoder LSTM Model\n",
    "\n",
    "Benchmark 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()\n",
    "\n",
    "df['A_diff'] = df['A'].diff()\n",
    "\n",
    "# Considering data with five features: A, C, G, A_diff, month\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff']] # remember the index for month\n",
    "daily_data.fillna(0, inplace=True)\n",
    "\n",
    "train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=[0, 1, 2, 3], n_gap=7, n_out=4, week=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=[0, 1, 2, 3], n_gap=7, n_out=4, week=50)\n",
    "\n",
    "model = build_lstm_v1_model(train, train_label, n_input, lstm_filter=64, dense_filter_decoder=12,\n",
    "                            epochs=35, n_out=4, n_gap=7)\n",
    "\n",
    "predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "lstm_rmse_v1 = np.sqrt(np.square(predictions-observations).mean(axis=0))\n",
    "\n",
    "print(lstm_rmse_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_lstm_v2_model(train, train_label, n_input, lstm_filter=64, dense_filter_decoder=12,\n",
    "                            epochs=30, n_out=4, n_gap=7)\n",
    "\n",
    "predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "lstm_rmse_v2 = np.sqrt(np.square(predictions-observations).mean(axis=0))\n",
    "\n",
    "print(lstm_rmse_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_lstm_v3_model(train, train_label, n_input, lstm_filter=64, dense_filter_decoder=6,\n",
    "                            epochs=30, n_out=4, n_gap=7)\n",
    "\n",
    "predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "lstm_rmse_v3 = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark 1 optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.utils.utils import optimization_process\n",
    "\n",
    "\n",
    "def evaluation_model(model, train, test, label_train, label_test, n_input, n_out=6, n_gap=7):\n",
    "    \n",
    "    history = [x for x in train[:-(n_out + n_gap)]]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in range(len(test) - (n_out + n_gap)):\n",
    "        history.append(test[i, :])\n",
    "        yhat_sequence = forecast(model, history, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap + 1) * 7: (i + n_out + n_gap + 1) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations\n",
    "\n",
    "\n",
    "def lstm_training_process(epochs, lstm_filter, dense_filter_decoder,\n",
    "                          learning_rate, beta_1, beta_2, epsilon):\n",
    "    \n",
    "    \n",
    "    n_input = 28\n",
    "    n_out = 4\n",
    "    n_gap = 7\n",
    "    \n",
    "    train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=[0,1,2,3], week=50, n_gap=7, n_out=4)\n",
    "    \n",
    "    model = build_lstm_v1_model(train, train_label, n_input=n_input, lstm_filter=int(lstm_filter), \n",
    "                                dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                                beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs),\n",
    "                                n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "    predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input, n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "    mse = mean_squared_error(observations, predictions)\n",
    "    \n",
    "    return -mse\n",
    "\n",
    "pbounds = {'epochs': (10, 40),\n",
    "           'lstm_filter': (48, 130),\n",
    "           'dense_filter_decoder': (8, 20),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01)}\n",
    "\n",
    "lstm_v1_optimized_parameters = optimization_process(lstm_training_process, pbounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_v1_optimized_parameters['dense_filter_decoder'] = int(lstm_v1_optimized_parameters['dense_filter_decoder'])\n",
    "lstm_v1_optimized_parameters['epochs'] = int(lstm_v1_optimized_parameters['epochs'])\n",
    "lstm_v1_optimized_parameters['lstm_filter'] = int(lstm_v1_optimized_parameters['lstm_filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_lstm_v1_model(train, train_label, n_input, n_out=4, n_gap=7, **lstm_v1_optimized_parameters)\n",
    "\n",
    "predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input, n_out=4, n_gap=7)\n",
    "\n",
    "optimized_lstm_v1_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(predictions-observations).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_lstm_v1_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark 2 optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_training_process(epochs=30, dense_filter_decoder=12, lstm_filter=64, learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "    \n",
    "    n_input = 28\n",
    "    n_out = 4\n",
    "    n_gap = 7\n",
    "    \n",
    "    train, test, label_train, label_test = split_dataset(daily_data.values, numerical_columns_index=[0,1,2,3], week=50, n_gap=7, n_out=4)\n",
    "    \n",
    "    model = build_lstm_v2_model(train, train_label, n_input=n_input, lstm_filter=int(lstm_filter), \n",
    "                                dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                                beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs),\n",
    "                                n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "    predictions, observations = evaluation_model(model, train, test, label_train, label_test, n_input, n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "\n",
    "    return -mean_squared_error(observations, predictions)\n",
    "\n",
    "\n",
    "pbounds = {'epochs': (10, 40),\n",
    "           'lstm_filter': (48, 130),\n",
    "           'dense_filter_decoder': (8, 20),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01)\n",
    "          }\n",
    "\n",
    "lstm_v2_optimized_parameters = optimization_process(lstm_training_process, pbounds)\n",
    "\n",
    "lstm_v2_optimized_parameters['dense_filter_decoder'] = int(lstm_v2_optimized_parameters['dense_filter_decoder'])\n",
    "lstm_v2_optimized_parameters['epochs'] = int(lstm_v2_optimized_parameters['epochs'])\n",
    "lstm_v2_optimized_parameters['lstm_filter'] = int(lstm_v2_optimized_parameters['lstm_filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_lstm_v2_model(train, train_label, n_input, n_out=4, n_gap=7, **lstm_v2_optimized_parameters)\n",
    "\n",
    "predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input, n_out=4, n_gap=7)\n",
    "\n",
    "optimized_lstm_v2_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(predictions-observations).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_lstm_v2_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark 3 optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_training_process(epochs=30, dense_filter_decoder=12, lstm_filter=64, learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "    \n",
    "    n_input = 28\n",
    "    n_out = 4\n",
    "    n_gap = 7\n",
    "    \n",
    "    train, test, label_train, label_test = split_dataset(daily_data.values, numerical_columns_index=[0,1,2,3], week=50, n_gap=7, n_out=4)\n",
    "    \n",
    "    model = build_lstm_v3_model(train, train_label, n_input=n_input, lstm_filter=int(lstm_filter), \n",
    "                                dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                                beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs),\n",
    "                                n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "    predictions, observations = evaluation_model(model, train, test, label_train, label_test, n_input, n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "\n",
    "    return -mean_squared_error(observations, predictions)\n",
    "\n",
    "\n",
    "pbounds = {'epochs': (10, 40),\n",
    "           'lstm_filter': (48, 130),\n",
    "           'dense_filter_decoder': (8, 20),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01)\n",
    "          }\n",
    "\n",
    "lstm_v3_optimized_parameters = optimization_process(lstm_training_process, pbounds)\n",
    "\n",
    "lstm_v3_optimized_parameters['dense_filter_decoder'] = int(lstm_v3_optimized_parameters['dense_filter_decoder'])\n",
    "lstm_v3_optimized_parameters['epochs'] = int(lstm_v3_optimized_parameters['epochs'])\n",
    "lstm_v3_optimized_parameters['lstm_filter'] = int(lstm_v3_optimized_parameters['lstm_filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_lstm_v3_model(train, train_label, n_input, n_out=4, n_gap=7, **lstm_v3_optimized_parameters)\n",
    "\n",
    "predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input, n_out=4, n_gap=7)\n",
    "\n",
    "optimized_lstm_v3_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(predictions-observations).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_lstm_v3_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "365 days baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values, numerical_columns_index=[0, 1, 2, 3], n_out=4, n_gap=7, week=50)\n",
    "\n",
    "prediction_input = label_train[:-(n_out + n_gap) * 7]\n",
    "\n",
    "predictions = list()\n",
    "observations = list()\n",
    "\n",
    "for i in tqdm(range(len(test) - (n_out + n_gap))):\n",
    "    \n",
    "    prediction_input = np.concatenate((prediction_input, label_test[:7]))\n",
    "    \n",
    "    yhat = np.array([np.mean(prediction_input[-365:]) * 7] * 4)\n",
    "    y = label_test[(1 + n_gap) * 7: (1 + n_gap + n_out) * 7]  # in days\n",
    "    y = np.array(np.split(y, n_out)).sum(axis=1)\n",
    "    \n",
    "    predictions.append(yhat)\n",
    "    observations.append(y)\n",
    "    \n",
    "    label_test = label_test[7:]\n",
    "    \n",
    "predictions = np.array(predictions)\n",
    "oobservations = np.array(observations)\n",
    "\n",
    "baseline_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(np.arange(4), baseline_rmse, '-o', label='365 days prediction')\n",
    "ax.plot(np.arange(4), optimized_lstm_v1_rmse, '-o', label='LSTM_v1')\n",
    "ax.plot(np.arange(4), optimized_lstm_v2_rmse, '-o', label='LSTM_v2')\n",
    "ax.plot(np.arange(4), optimized_lstm_v3_rmse, '-o', label='LSTM_v3')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('RMSE (100k)')\n",
    "ax.set_xticks(np.arange(4))\n",
    "ax.set_xticklabels(np.arange(4) + 1)\n",
    "ax.set_xlabel(\"7 + n Week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Time embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week'] = [idx.week for idx in df.index]\n",
    "df['month'] = [idx.month for idx in df.index]\n",
    "\n",
    "daily_data = df[['A', 'C', 'G', 'A_diff', 'week', 'month']] # remember the index for month\n",
    "daily_data.fillna(0, inplace=True)\n",
    "\n",
    "numerical_columns_index=[0, 1, 2, 3]\n",
    "\n",
    "train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=numerical_columns_index, n_out=4, n_gap=7, week=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "\n",
    "\n",
    "def to_supervised(train, train_label, n_input: int, n_out: int, n_gap: int,\n",
    "                  numerical_columns_index=None, export_future_time_info: bool = False):\n",
    "    '''\n",
    "    Args:\n",
    "        n_input: days\n",
    "        n_out: measured in weeks\n",
    "        n_future: measured in weeks\n",
    "    '''\n",
    "\n",
    "    # Multivariant input\n",
    "    # flatten data\n",
    "\n",
    "    X, y, X_weekly = list(), list(), list()\n",
    "    data = train.reshape((train.shape[0] * train.shape[1], train.shape[2]))\n",
    "    \n",
    "    if export_future_time_info:\n",
    "        data_timestamp = np.delete(data, numerical_columns_index, axis=1)\n",
    "        data = data[:, numerical_columns_index]\n",
    "        X_timestamp = list()\n",
    "    \n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data) - (n_out + n_gap) * 7):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_start = in_end + 7 * n_gap\n",
    "        out_end = out_start + 7 * n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[in_start:in_end, :])\n",
    "            y.append(np.array(np.split(train_label[out_start: out_end], n_out)).sum(axis=1))\n",
    "            X_weekly.append(np.array(np.split(data[in_start:in_end, :], n_out)).sum(axis=1))\n",
    "            if export_future_time_info:\n",
    "                X_timestamp.append(np.array(np.split(data_timestamp[out_start:out_end, :], n_out))[:, 0])\n",
    "        # add another week\n",
    "        in_start += 7\n",
    "\n",
    "    if export_future_time_info:\n",
    "        return np.array(X), np.array(y), np.array(X_weekly), np.array(X_timestamp)\n",
    "    else:\n",
    "        return np.array(X), np.array(y), np.array(X_weekly)\n",
    "    \n",
    "\n",
    "def forecast(model, history, timestamp_history, numerical_columns_index, n_input, n_out):\n",
    "    # flatten data\n",
    "    data_history = np.array(history)\n",
    "    h, w, c = data_history.shape\n",
    "    data_history = data_history.reshape((h*w, c))\n",
    "    \n",
    "    # retrieve last observations for input data\n",
    "    input_x = data_history[-n_input:, numerical_columns_index]\n",
    "    # reshape into [1, n_input, n_feature] Multivariant input\n",
    "    input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "    \n",
    "    # weekly aggregated data\n",
    "    input_x_weekly = data_history[-n_input:, :]\n",
    "    external = np.array(np.split(input_x_weekly, n_out)).sum(axis=1)\n",
    "    input_x_weekly = np.expand_dims(external, axis=0)\n",
    "    \n",
    "    # time data\n",
    "    data_timestamp_history = np.array(timestamp_history)\n",
    "    h, w, c = data_timestamp_history.shape\n",
    "    data_timestamp_history = data_timestamp_history.reshape((h*w, c))\n",
    "    \n",
    "    input_x_time = data_timestamp_history[-n_input:]\n",
    "    input_x_time = np.array(np.split(input_x_time, n_out))   # shape (4, 7, n_time_features)\n",
    "    input_x_time = np.expand_dims(input_x_time, axis=0)[:, :, 0, :]   # considering only the first day of the week as a proxy\n",
    "    \n",
    "    # forecast the next week\n",
    "    yhat = model.predict([input_x, input_x_time, input_x_weekly], verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def evaluation_model(train, test, label_train, label_test, numerical_columns_index, n_input, n_out=6, n_gap=7):\n",
    "    \n",
    "    model = build_lstm_embedding_model(train, label_train, numerical_columns_index, n_input=n_input, n_out=n_out, n_gap=n_gap)\n",
    "    history = [x for x in train[:-(n_out + n_gap), :, numerical_columns_index]]\n",
    "    \n",
    "    # process timestamp info for the targeet week\n",
    "    \n",
    "    timestamp_history = np.delete(train, numerical_columns_index, axis=2)\n",
    "    timestamp_history = [x for x in timestamp_history[n_out + n_gap:]]\n",
    "    \n",
    "    timestamp_future = np.delete(test, numerical_columns_index, axis=2)\n",
    "    timestamp_future = [x for x in timestamp_future[n_out + n_gap:]]\n",
    "    \n",
    "    # keep only numerical features\n",
    "    test = test[:, :, numerical_columns_index]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in tqdm(range(len(test) - (n_out + n_gap))):\n",
    "        history.append(test[i])  # for data generated in the past\n",
    "        timestamp_history.append(timestamp_future[i])\n",
    "        yhat_sequence = forecast(model, history, timestamp_history, numerical_columns_index, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap + 1) * 7: (i + n_out + n_gap + 1) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations\n",
    "\n",
    "\n",
    "def build_lstm_embedding_model(train, train_label, numerical_columns_index, n_input, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly, train_x_time = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap, numerical_columns_index=numerical_columns_index, export_future_time_info=True) \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0, 15, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # define model\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x, state_h, state_c = LSTM(24, activation='relu', return_state=True, name='LSTM_encoder_1', dropout=0.1, recurrent_dropout=0.1)(main_inputs)\n",
    "\n",
    "    decoder_input = RepeatVector(n_out, name='Repeat_hidden_state')(x)\n",
    "\n",
    "    y_hidden = LSTM(24, activation='relu', return_sequences=True, name='LSTM_decoder_1', dropout=0.1, recurrent_dropout=0.1)(decoder_input, initial_state=[state_h, state_c])\n",
    "#     y_hidden = tf.expand_dims(y_hidden, axis=2)\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, train_x_weekly.shape[2]), name='weekly_inputs')\n",
    "    y_weekly = LSTM(24, activation='relu', return_sequences=True, name='LSTM_decoder_2', dropout=0.1, recurrent_dropout=0.1)(weekly_inputs, initial_state=[state_h, state_c])\n",
    "#     y_weekly = tf.expand_dims(y_weekly , axis=2)\n",
    "    \n",
    "    time_inputs = Input(shape=(n_outputs, train_x_time.shape[2]), name='time_inputs')\n",
    "    y_embedding = Embedding(1000, 24, name='Time_Embedding')(time_inputs)\n",
    "    y_embedding = TimeDistributed(GlobalAveragePooling1D())(y_embedding)\n",
    "    y_embedding = LSTM(24, activation='relu', return_sequences=True, name='LSTM_decoder_3', dropout=0.1, recurrent_dropout=0.1)(y_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "    y = Concatenate(axis=2)([y_hidden, y_weekly, y_embedding])\n",
    "    \n",
    "    # reduce the shape to (none, n_out, n_of_features)\n",
    "    y = TimeDistributed(Dense(12, activation='relu', name='Time_distributed_dense_1'))(y)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(y)\n",
    "\n",
    "    model = Model(inputs=[main_inputs, time_inputs, weekly_inputs], outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'time_inputs': train_x_time, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_index=[0, 1, 2, 3]\n",
    "\n",
    "train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=numerical_columns_index, n_out=4, n_gap=7, week=50)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, train_label, test_label, numerical_columns_index, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "\n",
    "lstm_embedding_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_embedding_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(observations, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model(model, train, test, label_train, label_test, numerical_columns_index, n_input, n_out=6, n_gap=7):\n",
    "    \n",
    "    history = [x for x in train[:-(n_out + n_gap), :, numerical_columns_index]]\n",
    "    \n",
    "    # process timestamp info for the targeet week\n",
    "    \n",
    "    timestamp_history = np.delete(train, numerical_columns_index, axis=2)\n",
    "    timestamp_history = [x for x in timestamp_history[n_out + n_gap:]]\n",
    "    \n",
    "    timestamp_future = np.delete(test, numerical_columns_index, axis=2)\n",
    "    timestamp_future = [x for x in timestamp_future[n_out + n_gap:]]\n",
    "    \n",
    "    # keep only numerical features\n",
    "    test = test[:, :, numerical_columns_index]\n",
    "    \n",
    "    predictions = list()\n",
    "    observations = list()\n",
    "    \n",
    "    for i in range(len(test) - (n_out + n_gap)):\n",
    "        history.append(test[i])  # for data generated in the past\n",
    "        timestamp_history.append(timestamp_future[i])\n",
    "        yhat_sequence = forecast(model, history, timestamp_history, numerical_columns_index, n_input, n_out)\n",
    "        predictions.append(yhat_sequence)\n",
    "        observation = np.split(label_test[(i + n_gap + 1) * 7: (i + n_out + n_gap + 1) * 7], n_out)\n",
    "        observations.append(np.array(observation).sum(axis=1))\n",
    "    predictions = np.array(predictions)[:, :, 0]\n",
    "    observations = np.array(observations)\n",
    "    \n",
    "    return predictions, observations\n",
    "\n",
    "\n",
    "def build_lstm_embedding_model(train, train_label, numerical_columns_index, n_input, lstm_filter, dense_filter_decoder, \n",
    "                               learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, epochs=15, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly, train_x_time = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap, numerical_columns_index=numerical_columns_index, export_future_time_info=True) \n",
    "    # define parameters\n",
    "    verbose, batch_size = 0, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # define model\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x, state_h, state_c = LSTM(lstm_filter, activation='relu', return_state=True, name='LSTM_encoder_1', dropout=0.1, recurrent_dropout=0.1)(main_inputs)\n",
    "\n",
    "    decoder_input = RepeatVector(n_out, name='Repeat_hidden_state')(x)\n",
    "\n",
    "    y_hidden = LSTM(lstm_filter, activation='relu', return_sequences=True, name='LSTM_decoder_1', dropout=0.1, recurrent_dropout=0.1)(decoder_input, initial_state=[state_h, state_c])\n",
    "    y_hidden = tf.expand_dims(y_hidden, axis=2)\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, train_x_weekly.shape[2]), name='weekly_inputs')\n",
    "    y_weekly = LSTM(lstm_filter, activation='relu', return_sequences=True, name='LSTM_decoder_2', dropout=0.1, recurrent_dropout=0.1)(weekly_inputs, initial_state=[state_h, state_c])\n",
    "    y_weekly = tf.expand_dims(y_weekly , axis=2)\n",
    "    \n",
    "    time_inputs = Input(shape=(n_outputs, train_x_time.shape[2]), name='time_inputs')\n",
    "    y_embedding = Embedding(1000, lstm_filter, name='Time_Embedding')(time_inputs)\n",
    "#     y_embedding = LSTM(lstm_filter, activation='relu', return_sequences=True, name='LSTM_decoder_3', dropout=0.1, recurrent_dropout=0.1)(y_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "    y = Concatenate(axis=2)([y_hidden, y_weekly, y_embedding])\n",
    "#     y = Conv1D(filters=lstm_filter, kernel_size=3, activation='relu', padding='same')(y)\n",
    "    y = TimeDistributed(GlobalAveragePooling1D())(y)\n",
    "    \n",
    "    # reduce the shape to (none, n_out, n_of_features)\n",
    "    y = TimeDistributed(Dense(dense_filter_decoder, activation='relu', name='Time_distributed_dense_1'))(y)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(y)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, time_inputs, weekly_inputs], outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'time_inputs': train_x_time, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model\n",
    "\n",
    "\n",
    "def lstm_training_process(epochs, lstm_filter, dense_filter_decoder,\n",
    "                          learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "    \n",
    "    \n",
    "    n_input = 28\n",
    "    n_out = 4\n",
    "    n_gap = 7\n",
    "    numerical_columns_index=[0,1,2,3]\n",
    "    \n",
    "    train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=numerical_columns_index, week=50, n_gap=n_gap, n_out=n_out)\n",
    "    \n",
    "    model = build_lstm_embedding_model(train, train_label, numerical_columns_index=numerical_columns_index, n_input=n_input, lstm_filter=int(lstm_filter), \n",
    "                                       dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                                       beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs),\n",
    "                                       n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "    predictions, observations = evaluation_model(model, train, test, train_label, test_label, n_input=n_input, numerical_columns_index=numerical_columns_index, n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "    mse = mean_squared_error(observations, predictions)\n",
    "    \n",
    "    return -mse\n",
    "\n",
    "pbounds = {'epochs': (10, 30),\n",
    "           'lstm_filter': (16, 72),\n",
    "           'dense_filter_decoder': (4, 24),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "#            'beta_1': (0.5, 0.95),\n",
    "#            'beta_2': (0.7, 0.999),\n",
    "           'epsilon': (0.0001, 0.1)}\n",
    "\n",
    "lstm_embeding_optimized_parameters = optimization_process(lstm_training_process, pbounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_embeding_optimized_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_embeding_optimized_parameters['dense_filter_decoder'] = int(lstm_embeding_optimized_parameters['dense_filter_decoder'])\n",
    "lstm_embeding_optimized_parameters['epochs'] = int(lstm_embeding_optimized_parameters['epochs'])\n",
    "lstm_embeding_optimized_parameters['lstm_filter'] = int(lstm_embeding_optimized_parameters['lstm_filter'])\n",
    "\n",
    "numerical_columns_index=[0, 1, 2, 3]\n",
    "\n",
    "train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=numerical_columns_index, n_out=4, n_gap=7, week=50)\n",
    "\n",
    "model = build_lstm_embedding_model(train, train_label, n_input=28, numerical_columns_index=numerical_columns_index, n_out=4, n_gap=7, **lstm_embeding_optimized_parameters)\n",
    "\n",
    "predictions, observations = evaluation_model(model, train, test, train_label, test_label, numerical_columns_index=numerical_columns_index, n_input=28, n_out=4, n_gap=7)\n",
    "\n",
    "lstm_embedding_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_embedding_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(observations, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model stacking with Prophet\n",
    "\n",
    "Using Prophet removing time dependence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "scheduled_off = pd.to_datetime(['2018-02-15', '2018-02-16', '2018-02-17', '2018-02-18', '2018-12-30', '2018-12-31', '2019-01-01', \n",
    "                                '2019-02-02', '2019-02-03', '2019-02-04', '2019-02-05', '2019-02-06', '2019-02-07', '2019-02-08', \n",
    "                                '2019-02-09', '2019-02-10', '2019-12-30', '2019-12-31', '2020-01-23', '2020-01-24', '2020-01-25', \n",
    "                                '2020-01-26', '2020-01-27', '2021-01-01', '2021-02-11', '2021-02-12', '2021-02-13', '2021-02-14', \n",
    "                                '2019-04-05', '2019-04-06', '2019-04-07', '2016-02-06', '2016-02-07', '2016-02-08', '2016-02-09',\n",
    "                                '2016-02-10', '2016-04-04', '2016-09-16', '2016-12-29', '2016-12-30', '2016-12-31', '2017-01-27', \n",
    "                                '2017-01-28', '2017-01-29', '2017-01-30'])\n",
    "\n",
    "scheduled_off_df = pd.DataFrame({'holiday': 'scheduled_off',\n",
    "                                 'ds': scheduled_off,\n",
    "                                 'lower_window': -2,  # range of impact of off days\n",
    "                                 'upper_window': 2})\n",
    "\n",
    "df = raw_data_preparation()\n",
    "# In order to achieve the specification of Prophet\n",
    "\n",
    "df.index.name = 'ds'  # specify timestamp index name as ds\n",
    "df.reset_index(inplace=True)  # make index into column\n",
    "df.rename(columns={'A': 'y'}, inplace=True)  # rename the target column as y\n",
    "\n",
    "# train-test split\n",
    "\n",
    "df_train = df.iloc[:-(week)*7]\n",
    "df_test = df.iloc[-(week+n_gap+n_out)*7:]\n",
    "\n",
    "m = Prophet(holidays=scheduled_off_df, \n",
    "            growth='linear',\n",
    "            changepoint_prior_scale=0.005,\n",
    "            changepoint_range=0.9,\n",
    "            yearly_seasonality=10,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False)\n",
    "\n",
    "#  Add quarter seasonality by hand\n",
    "#  Give the name of your seasonality and the corresponding period\n",
    "m.add_seasonality(name='month', period=30, fourier_order=5) \n",
    "m.fit(df_train.iloc[:-(n_gap + n_out) * 7])\n",
    "\n",
    "future_time = m.make_future_dataframe(len(df_test))\n",
    "prediction = m.predict(future_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = m.plot_components(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.iloc[-len(df_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.plot(df_test['y'].values, label='Observation')\n",
    "ax.plot(prediction.iloc[-len(df_test):]['yhat'].values, label='Prophet prediction')\n",
    "\n",
    "ax.set_xticks(np.arange(0, len(df_test), 50))\n",
    "ax.set_xticklabels([str(a)[:10] for a in df_test['ds'].values[0:len(df_test):50]])\n",
    "ax.set_title(\"Prophet Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A_prophet'] = prediction['yhat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A_residue'] = df['A'] - df['A_prophet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A_residue'].plot(figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_index=[0, 1, 2, 3]\n",
    "\n",
    "df['A_diff'] = df['A'].diff()\n",
    "\n",
    "# Considering data with five features: A, C, G, A_diff, month\n",
    "\n",
    "daily_data = df[['A_residue', 'C', 'G', 'A_diff']] # remember the index for month\n",
    "daily_data.fillna(0, inplace=True)\n",
    "\n",
    "train, test, train_label, test_label = split_dataset(daily_data.values, numerical_columns_index=[0, 1, 2, 3], n_gap=7, n_out=4, week=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_training_process(epochs=30, dense_filter_decoder=12, lstm_filter=64, learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
    "    \n",
    "    n_input = 28\n",
    "    n_out = 4\n",
    "    n_gap = 7\n",
    "    \n",
    "    train, test, label_train, label_test = split_dataset(daily_data.values, numerical_columns_index=[0,1,2,3], week=50, n_gap=7, n_out=4)\n",
    "    \n",
    "    model = build_lstm_v2_model(train, train_label, n_input=n_input, lstm_filter=int(lstm_filter), \n",
    "                                dense_filter_decoder=int(dense_filter_decoder), learning_rate=learning_rate, \n",
    "                                beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, epochs=int(epochs),\n",
    "                                n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "    predictions, observations = evaluation_model(model, train, test, label_train, label_test, n_input, n_out=n_out, n_gap=n_gap)\n",
    "    \n",
    "\n",
    "    return -mean_squared_error(observations, predictions)\n",
    "\n",
    "\n",
    "pbounds = {'epochs': (10, 40),\n",
    "           'lstm_filter': (48, 130),\n",
    "           'dense_filter_decoder': (8, 20),\n",
    "           'learning_rate': (0.0001, 0.003),\n",
    "           'beta_1': (0.5, 0.95),\n",
    "           'beta_2': (0.7, 0.9999),\n",
    "           'epsilon': (0.00001, 0.01)\n",
    "          }\n",
    "\n",
    "lstm_v2_optimized_parameters = optimization_process(lstm_training_process, pbounds)\n",
    "\n",
    "lstm_v2_optimized_parameters['dense_filter_decoder'] = int(lstm_v2_optimized_parameters['dense_filter_decoder'])\n",
    "lstm_v2_optimized_parameters['epochs'] = int(lstm_v2_optimized_parameters['epochs'])\n",
    "lstm_v2_optimized_parameters['lstm_filter'] = int(lstm_v2_optimized_parameters['lstm_filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(df_test)//7 - (n_out + n_gap))[:1]:\n",
    "    \n",
    "#     m = Prophet(holidays=scheduled_off_df, \n",
    "#                 growth='linear',\n",
    "#                 changepoint_prior_scale=0.005,\n",
    "#                 changepoint_range=0.9,\n",
    "#                 yearly_seasonality=10,\n",
    "#                 weekly_seasonality=True,\n",
    "#                 daily_seasonality=False)\n",
    "\n",
    "#     #  Add quarter seasonality by hand\n",
    "#     #  Give the name of your seasonality and the corresponding period\n",
    "#     m.add_seasonality(name='month', period=30, fourier_order=5) \n",
    "    \n",
    "#     m.fit(pd.concat((df_train.iloc[:-(n_gap + n_out) * 7], df_test.iloc[:(i+1) * 7])))\n",
    "#     future = m.make_future_dataframe(periods=(n_gap + n_out) * 7)\n",
    "#     prediction = m.predict(future)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, train_label, n_input, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 25, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', # 加權平均\n",
    "               input_shape=(n_timesteps,n_features))(main_inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x, state_h, state_c = LSTM(64, activation='relu', return_state=True)(x)\n",
    "    \n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs')\n",
    "    x = LSTM(64, activation='relu', return_sequences=True)(weekly_inputs, initial_state=[state_h, state_c])\n",
    "    x = TimeDistributed(Dense(12, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(x)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "cnnlstm_rmse = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, train_label, n_input, n_out=6, n_gap=7):\n",
    "    # prepare data\n",
    "    train_x, train_y, train_x_weekly = to_supervised(train, train_label, n_input, n_out=n_out, n_gap=n_gap) \n",
    "    \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 1, 25, 16\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    main_inputs = Input(shape=(n_timesteps, n_features), name='main_inputs')\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', \n",
    "               input_shape=(n_timesteps,n_features))(main_inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x, state_h, state_c = LSTM(64, activation='relu', return_state=True)(x)\n",
    "    decoder_input = RepeatVector(n_out)(x)\n",
    "    _, _, dim = train_x_weekly.shape\n",
    "    \n",
    "    weekly_inputs = Input(shape=(n_outputs, dim), name='weekly_inputs')\n",
    "    \n",
    "    decoder_input = Concatenate(axis=2)([weekly_inputs, decoder_input])\n",
    "    \n",
    "    x = LSTM(64, activation='elu', return_sequences=True, dropout=0.3, \n",
    "             recurrent_dropout=0.1)(decoder_input, initial_state=[state_h, state_c])\n",
    "    x = TimeDistributed(Dense(12, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(1), name='outputs')(x)\n",
    "    \n",
    "    model = Model(inputs=[main_inputs, weekly_inputs], outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit({'main_inputs': train_x, 'weekly_inputs': train_x_weekly}, \n",
    "              {'outputs': train_y}, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, label_train, label_test = split_dataset(daily_data.values)\n",
    "\n",
    "predictions, observations = evaluation_model(train, test, label_train, label_test, n_input=n_input,\n",
    "                                             n_out=n_out, n_gap=n_gap)\n",
    "# cnnlstm_rmse_v2 = np.sqrt(np.square(predictions-observations).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm_rmse_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet\n",
    "\n",
    "- Dependence on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "scheduled_off = pd.to_datetime(['2018-02-15', '2018-02-16', '2018-02-17', '2018-02-18', '2018-12-30', '2018-12-31', '2019-01-01', \n",
    "                                '2019-02-02', '2019-02-03', '2019-02-04', '2019-02-05', '2019-02-06', '2019-02-07', '2019-02-08', \n",
    "                                '2019-02-09', '2019-02-10', '2019-12-30', '2019-12-31', '2020-01-23', '2020-01-24', '2020-01-25', \n",
    "                                '2020-01-26', '2020-01-27', '2021-01-01', '2021-02-11', '2021-02-12', '2021-02-13', '2021-02-14', \n",
    "                                '2019-04-05', '2019-04-06', '2019-04-07', '2016-02-06', '2016-02-07', '2016-02-08', '2016-02-09',\n",
    "                                '2016-02-10', '2016-04-04', '2016-09-16', '2016-12-29', '2016-12-30', '2016-12-31', '2017-01-27', \n",
    "                                '2017-01-28', '2017-01-29', '2017-01-30'])\n",
    "\n",
    "scheduled_off_df = pd.DataFrame({'holiday': 'scheduled_off',\n",
    "                                 'ds': scheduled_off,\n",
    "                                 'lower_window': -2,  # range of impact of off days\n",
    "                                 'upper_window': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to achieve the specification of Prophet\n",
    "\n",
    "df.index.name = 'ds'  # specify timestamp index name as ds\n",
    "df.reset_index(inplace=True)  # make index into column\n",
    "df.rename(columns={'A': 'y'}, inplace=True)  # rename the target column as y\n",
    "\n",
    "# train-test split\n",
    "\n",
    "df_train = df.iloc[:-350]\n",
    "df_test = df.iloc[-350:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet also has built-in yearly, weekly, and daily seasonality\n",
    "# Because we have only consumptions per day, we also turned off daily seasonality\n",
    "\n",
    "m = Prophet(holidays=scheduled_off_df, \n",
    "            growth='linear',\n",
    "            changepoint_prior_scale=0.005,\n",
    "            changepoint_range=0.9,\n",
    "            yearly_seasonality=10,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False)\n",
    "\n",
    "#  Add quarter seasonality by hand\n",
    "#  Give the name of your seasonality and the corresponding period\n",
    "m.add_seasonality(name='month', period=30, fourier_order=5) \n",
    "m.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduled_off_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=350)\n",
    "\n",
    "prediction = m.predict(future)\n",
    "fig = m.plot_components(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.plot(df_test['y'].values, label='Observation')\n",
    "ax.plot(prediction.iloc[-350:]['yhat'].values, label='Prophet prediction')\n",
    "\n",
    "ax.set_xticks(np.arange(0, 350, 50))\n",
    "ax.set_xticklabels([str(a)[:10] for a in df_test['ds'].values[0:350:50]])\n",
    "ax.set_title(\"Prophet Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://blog.floydhub.com/content/images/2018/12/queen-man-woman.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "月，週，日，季(1-3, 4-6, 7-9, 10-12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
